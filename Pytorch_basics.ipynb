{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c855b0-c373-451a-a0ef-343967639f2c",
   "metadata": {},
   "source": [
    "#  - - - - - - - - - - - - - - Quickstart - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362d6dd5-cd76-4d8f-b41c-682ef40878b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f463fe-eb03-4142-a54c-51588a0acc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00bc66f-2836-45fe-910e-906e6795474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0505d6a-9ba3-4a29-89e7-7ea529597530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6b672c-e986-41be-be9e-30c3be5101fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0783fa0-895f-4f50-b61e-781906e64254",
   "metadata": {},
   "source": [
    "### Setting up loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbca6918-464f-4a4e-8520-476c5d15f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbdd92f-97b6-4a29-857f-f1a259879e02",
   "metadata": {},
   "source": [
    "### Setting up training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6769e199-6ef3-458e-8f05-43a6209b24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6eb73-6aa1-414b-9bd7-ecadf92c527d",
   "metadata": {},
   "source": [
    "### Setting up testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c171f96-ae90-4f1b-a6ba-62ab0d5b10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927acd6-f1c6-48cc-bd00-148e236095fb",
   "metadata": {},
   "source": [
    "### Training model and checking convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29557c3e-e2a7-4cad-8332-474a0ba1d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305623  [   64/60000]\n",
      "loss: 2.293790  [ 6464/60000]\n",
      "loss: 2.284343  [12864/60000]\n",
      "loss: 2.286017  [19264/60000]\n",
      "loss: 2.265083  [25664/60000]\n",
      "loss: 2.236182  [32064/60000]\n",
      "loss: 2.241286  [38464/60000]\n",
      "loss: 2.208699  [44864/60000]\n",
      "loss: 2.213473  [51264/60000]\n",
      "loss: 2.187771  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 2.181495 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.182659  [   64/60000]\n",
      "loss: 2.172243  [ 6464/60000]\n",
      "loss: 2.130452  [12864/60000]\n",
      "loss: 2.156027  [19264/60000]\n",
      "loss: 2.102840  [25664/60000]\n",
      "loss: 2.045969  [32064/60000]\n",
      "loss: 2.074126  [38464/60000]\n",
      "loss: 2.002542  [44864/60000]\n",
      "loss: 2.011498  [51264/60000]\n",
      "loss: 1.951326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.946757 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.964424  [   64/60000]\n",
      "loss: 1.937782  [ 6464/60000]\n",
      "loss: 1.841009  [12864/60000]\n",
      "loss: 1.891916  [19264/60000]\n",
      "loss: 1.772267  [25664/60000]\n",
      "loss: 1.716687  [32064/60000]\n",
      "loss: 1.740766  [38464/60000]\n",
      "loss: 1.641510  [44864/60000]\n",
      "loss: 1.665585  [51264/60000]\n",
      "loss: 1.566357  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.578637 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.632479  [   64/60000]\n",
      "loss: 1.597528  [ 6464/60000]\n",
      "loss: 1.457605  [12864/60000]\n",
      "loss: 1.536627  [19264/60000]\n",
      "loss: 1.403397  [25664/60000]\n",
      "loss: 1.390100  [32064/60000]\n",
      "loss: 1.406158  [38464/60000]\n",
      "loss: 1.325092  [44864/60000]\n",
      "loss: 1.359516  [51264/60000]\n",
      "loss: 1.267423  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.287094 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.357507  [   64/60000]\n",
      "loss: 1.338923  [ 6464/60000]\n",
      "loss: 1.178495  [12864/60000]\n",
      "loss: 1.291532  [19264/60000]\n",
      "loss: 1.156637  [25664/60000]\n",
      "loss: 1.176730  [32064/60000]\n",
      "loss: 1.199611  [38464/60000]\n",
      "loss: 1.128712  [44864/60000]\n",
      "loss: 1.167194  [51264/60000]\n",
      "loss: 1.095975  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.108729 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b543c15-ef5e-4c24-9420-92eef84d76f1",
   "metadata": {},
   "source": [
    "### Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7d5a93-c503-4821-9d32-48b86dc17570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to Trained_models/first_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"Trained_models/first_model.pth\")\n",
    "print(\"Saved PyTorch Model State to Trained_models/first_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a277b506-c7a6-4dfc-a6b4-b4f670215445",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "090ead6a-1e8f-4a07-bb63-03daba7ccaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"Trained_models/first_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729300aa-2075-4820-aef7-7c43a53bf620",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7853eb-1ac7-4972-9607-8de4285ef324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted = classes[pred[0].argmax(0)]\n",
    "    actual = classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14333f3b-b531-42d2-b9f3-f220f2df3502",
   "metadata": {},
   "source": [
    "#  - - - - - - - - - - - - - - Tensors - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2397754-283d-4263-a0b1-174da5928768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f0d46-f06d-4059-b17c-ed88fee51c34",
   "metadata": {},
   "source": [
    "### Tensor initialization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5864327a-3b98-4d7d-9a0a-5f9c4e44a499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d0a4db-11d5-41dd-81f7-c609d6ea4e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28ded06a-5ac1-496e-a390-f0089d240d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9493, 0.9896],\n",
      "        [0.0750, 0.7037]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed3a3945-be1d-4b37-94e4-1a91a9007c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.3354, 0.4093, 0.4206],\n",
      "        [0.0176, 0.0095, 0.5188]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67414f-2534-4340-99d1-3f528aeb3ba3",
   "metadata": {},
   "source": [
    "### Tensor attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd88fe3f-8263-46ad-9974-a76cad22e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9e3a4-943b-4945-9cd0-a2a54cbefdeb",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda8d729-45ec-4e23-9525-4a059a1f467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cbdac28-c682-4947-8f6a-218d2ce761c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec3eaed1-b224-4f8c-8d44-40ec11162852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2107f75-2bd1-4c58-954b-9f5c46f2e1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d23864-cc29-4de2-9305-23802bdf3bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c728b81-8e33-4767-aad2-c1e846a354b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90eb87-d574-4086-b44a-eec2b77593ef",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "    \n",
    "    In-place operations save some memory,\n",
    "    but can be problematic when computing derivatives because of an immediate loss of history. \n",
    "    Hence, their use is discouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "143bca2f-73ab-46ac-9da7-117fcc37801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9f5d5c4-d21d-476d-9291-b879524fb8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c174a4f9-b23e-420b-bb3f-8f4b3d552ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df2957ff-1492-4690-95af-0fbbd250eaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99d682-277b-49d8-8bd7-f979afc641eb",
   "metadata": {},
   "source": [
    "# Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c8191e5-a70e-45aa-a9b0-d8d449bf7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5eb947f-2c00-4b8d-916b-924208630c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(),cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968a938-2a77-444a-a34e-ea0891730736",
   "metadata": {},
   "source": [
    "# Preparing your data for training with DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36c3a2a9-d26b-4548-95c8-ac259da08f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beddbc6f-901f-4bb4-acd0-f04659e40a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d847f6-cfd0-4641-a128-3089bbca9bef",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcb836-1e4f-4e51-ae8b-8fe72630c431",
   "metadata": {},
   "source": [
    "The FashionMNIST features are in PIL Image format, and the labels are integers. For training, we need the features as normalized tensors,\n",
    "and the labels as one-hot encoded tensors.\n",
    "To make these transformations, we use ToTensor and Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e375421a-f32a-4558-b266-cf40347f19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473df41d-cd4e-4183-8a41-a12ea9df454b",
   "metadata": {},
   "source": [
    "# Build a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ec47d4d-8e3a-4a8e-b574-cfd85dee184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e138715-fb96-4274-8f5f-ca6d1844f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1 device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda:1\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa76516-125d-4000-8f0a-aa6e54bd1314",
   "metadata": {},
   "source": [
    "### Network class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3be741f1-ffb2-407a-adaf-31e81f7c3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3113b807-3585-4dcd-8b4d-d800d07da1bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1160\u001b[0m         device,\n\u001b[0;32m   1161\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1162\u001b[0m         non_blocking,\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3811919-4d47-4cde-8f2e-2dd9efc2425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af23854-e784-463b-8fab-0a832f0e5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58283d-1a4e-4b52-ae17-51c7dadb757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0866b33-653c-432e-adfc-6afdc8ac1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b5037d2-a722-4eb9-b839-dd6a52cd5612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[0.0000, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0473, 0.0565, 0.0000,\n",
      "         0.0000, 0.0000, 0.0623, 0.0000, 0.0953, 0.1988, 0.0000, 0.0000, 0.0000,\n",
      "         0.0067, 0.0000],\n",
      "        [0.1973, 0.0011, 0.0000, 0.0000, 0.0000, 0.0959, 0.0000, 0.5061, 0.2791,\n",
      "         0.0962, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0063, 0.3092],\n",
      "        [0.4153, 0.2350, 0.0000, 0.0000, 0.0000, 0.0173, 0.0000, 0.4724, 0.0000,\n",
      "         0.0000, 0.0029, 0.0485, 0.0000, 0.0000, 0.3673, 0.1260, 0.0000, 0.0000,\n",
      "         0.2177, 0.5280]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0473, 0.0565, 0.0000,\n",
      "         0.0000, 0.0000, 0.0623, 0.0000, 0.0953, 0.1988, 0.0000, 0.0000, 0.0000,\n",
      "         0.0067, 0.0000],\n",
      "        [0.1973, 0.0011, 0.0000, 0.0000, 0.0000, 0.0959, 0.0000, 0.5061, 0.2791,\n",
      "         0.0962, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0063, 0.3092],\n",
      "        [0.4153, 0.2350, 0.0000, 0.0000, 0.0000, 0.0173, 0.0000, 0.4724, 0.0000,\n",
      "         0.0000, 0.0029, 0.0485, 0.0000, 0.0000, 0.3673, 0.1260, 0.0000, 0.0000,\n",
      "         0.2177, 0.5280]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cdba3b89-5094-4449-9882-23cbf201854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66fe58c1-2b0b-4d69-bc01-ccf9e0f22818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "276359df-4531-4886-8ffb-1572beb66a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0043,  0.0107, -0.0010,  ...,  0.0056,  0.0147, -0.0273],\n",
      "        [-0.0296, -0.0086, -0.0095,  ...,  0.0101,  0.0155,  0.0139]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0118, -0.0123], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0405, -0.0133, -0.0277,  ...,  0.0410,  0.0068,  0.0108],\n",
      "        [ 0.0103,  0.0323, -0.0132,  ...,  0.0400,  0.0341,  0.0191]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0152, 0.0098], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0374,  0.0691, -0.0122,  ..., -0.0315, -0.0097,  0.0018],\n",
      "        [ 0.0257,  0.0009,  0.0257,  ..., -0.1017,  0.0353, -0.1572]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0170, -0.0180], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c6382-670c-464e-9a25-5b3d264a5e5e",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fcd6d6b4-a287-4524-9cd1-9276d10760d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dabd7bba-402e-4934-80bf-2517a96cc029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000002A1B34B6FE0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000002A1C11F6C80>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4ff48ed-a5b9-41ee-a0cf-02208f5733e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0081, 0.0102, 0.2156],\n",
      "        [0.0081, 0.0102, 0.2156],\n",
      "        [0.0081, 0.0102, 0.2156],\n",
      "        [0.0081, 0.0102, 0.2156],\n",
      "        [0.0081, 0.0102, 0.2156]])\n",
      "tensor([0.0081, 0.0102, 0.2156])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f7ed6-907d-4284-bf30-808e867f6135",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "We can only obtain the grad properties for the leaf nodes of the computational graph, \n",
    "which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "\n",
    "We can only perform gradient calculations using backward once on a given graph,\n",
    "for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c08ec7c3-8332-434d-87df-63635989d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91a77dde-c671-4d40-94a9-6dbbf1118ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe38d0-8211-4d65-9f8c-347b3935b75b",
   "metadata": {},
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    " - To mark some parameters in your neural network as frozen parameters.\n",
    "\n",
    " - To speed up computations when you are only doing forward pass,\n",
    "because computations on tensors that do not track gradients would be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af375be-c137-449e-bf29-46bca38e9a2f",
   "metadata": {},
   "source": [
    "NOTE\n",
    "\n",
    " - Previously we were calling backward() function without parameters. This is essentially equivalent to calling backward(torch.tensor(1.0)), \n",
    "which is a useful way to compute the gradients in case of a scalar-valued function,\n",
    "such as loss during neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b633e62-b386-4b6a-a6cc-26c1ee5138d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1055e-efe7-49ee-9546-9de85b22f253",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a5c90b7-039c-4296-90c6-08cd97e1e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853836d9-59cc-4c8a-9627-f5e7579905a6",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8447881c-535a-4c7a-b791-f2613d4a927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5 # not used actually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7e43d-d8b1-414e-bfe6-002b3da23a75",
   "metadata": {},
   "source": [
    "### Optimization Loop\n",
    "Each epoch consists of two main parts:\n",
    "\n",
    "\n",
    "The Train Loop - iterate over the training dataset and try to converge to optimal parameters\n",
    "  .\n",
    "\r\n",
    "The Validation/Test Loop - iterate over the test dataset to check if model performance is improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634319b2-3b8f-437d-9453-a4c73a132c28",
   "metadata": {},
   "source": [
    "### Setup Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a06f7617-1c88-4f11-b9ae-56c80eca8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfd29d-eed6-48e6-b58c-f0dba8063994",
   "metadata": {},
   "source": [
    "### Setup Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ef6be5d-e356-401e-a686-84dd34a9e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1a6a8-bd37-4543-b791-141d03a019aa",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "1. Call optimizer.zero_grad() to reset the gradients of model parameters. \n",
    "Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "\n",
    "2. Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "\n",
    "3. Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0af5e-7fae-40d8-83f8-f833edda4f61",
   "metadata": {},
   "source": [
    "### Train loop and test loop full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "29074c4c-ffc3-422e-b6a4-9522f2b5e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e66d3-3f7d-44f4-af9b-c09bb251947f",
   "metadata": {},
   "source": [
    "### Full learning testing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b09e9acf-0d61-433b-81a0-54d163be9a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.292016  [   64/60000]\n",
      "loss: 2.295952  [ 6464/60000]\n",
      "loss: 2.272824  [12864/60000]\n",
      "loss: 2.274248  [19264/60000]\n",
      "loss: 2.260049  [25664/60000]\n",
      "loss: 2.214980  [32064/60000]\n",
      "loss: 2.236586  [38464/60000]\n",
      "loss: 2.200264  [44864/60000]\n",
      "loss: 2.191515  [51264/60000]\n",
      "loss: 2.160996  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 2.164093 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.169984  [   64/60000]\n",
      "loss: 2.172561  [ 6464/60000]\n",
      "loss: 2.111219  [12864/60000]\n",
      "loss: 2.132084  [19264/60000]\n",
      "loss: 2.089107  [25664/60000]\n",
      "loss: 2.015428  [32064/60000]\n",
      "loss: 2.063778  [38464/60000]\n",
      "loss: 1.982461  [44864/60000]\n",
      "loss: 1.987002  [51264/60000]\n",
      "loss: 1.915203  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.914706 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.947199  [   64/60000]\n",
      "loss: 1.928544  [ 6464/60000]\n",
      "loss: 1.801282  [12864/60000]\n",
      "loss: 1.847003  [19264/60000]\n",
      "loss: 1.738319  [25664/60000]\n",
      "loss: 1.677485  [32064/60000]\n",
      "loss: 1.717034  [38464/60000]\n",
      "loss: 1.604493  [44864/60000]\n",
      "loss: 1.632204  [51264/60000]\n",
      "loss: 1.520728  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.535846 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.606872  [   64/60000]\n",
      "loss: 1.575106  [ 6464/60000]\n",
      "loss: 1.406838  [12864/60000]\n",
      "loss: 1.486038  [19264/60000]\n",
      "loss: 1.359090  [25664/60000]\n",
      "loss: 1.351643  [32064/60000]\n",
      "loss: 1.371812  [38464/60000]\n",
      "loss: 1.287315  [44864/60000]\n",
      "loss: 1.324336  [51264/60000]\n",
      "loss: 1.217532  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.248601 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.331798  [   64/60000]\n",
      "loss: 1.317037  [ 6464/60000]\n",
      "loss: 1.134894  [12864/60000]\n",
      "loss: 1.248500  [19264/60000]\n",
      "loss: 1.121269  [25664/60000]\n",
      "loss: 1.146171  [32064/60000]\n",
      "loss: 1.167045  [38464/60000]\n",
      "loss: 1.101727  [44864/60000]\n",
      "loss: 1.139064  [51264/60000]\n",
      "loss: 1.051451  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.079491 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.153907  [   64/60000]\n",
      "loss: 1.161226  [ 6464/60000]\n",
      "loss: 0.964314  [12864/60000]\n",
      "loss: 1.108501  [19264/60000]\n",
      "loss: 0.985428  [25664/60000]\n",
      "loss: 1.014585  [32064/60000]\n",
      "loss: 1.048578  [38464/60000]\n",
      "loss: 0.990641  [44864/60000]\n",
      "loss: 1.025030  [51264/60000]\n",
      "loss: 0.952828  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.975065 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.034651  [   64/60000]\n",
      "loss: 1.063047  [ 6464/60000]\n",
      "loss: 0.851157  [12864/60000]\n",
      "loss: 1.018588  [19264/60000]\n",
      "loss: 0.903909  [25664/60000]\n",
      "loss: 0.923379  [32064/60000]\n",
      "loss: 0.973364  [38464/60000]\n",
      "loss: 0.920579  [44864/60000]\n",
      "loss: 0.948358  [51264/60000]\n",
      "loss: 0.887494  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.904914 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.947995  [   64/60000]\n",
      "loss: 0.995260  [ 6464/60000]\n",
      "loss: 0.771210  [12864/60000]\n",
      "loss: 0.955739  [19264/60000]\n",
      "loss: 0.850017  [25664/60000]\n",
      "loss: 0.856810  [32064/60000]\n",
      "loss: 0.920770  [38464/60000]\n",
      "loss: 0.874202  [44864/60000]\n",
      "loss: 0.893606  [51264/60000]\n",
      "loss: 0.840144  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.854352 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.881373  [   64/60000]\n",
      "loss: 0.944438  [ 6464/60000]\n",
      "loss: 0.711931  [12864/60000]\n",
      "loss: 0.908927  [19264/60000]\n",
      "loss: 0.811592  [25664/60000]\n",
      "loss: 0.806731  [32064/60000]\n",
      "loss: 0.880926  [38464/60000]\n",
      "loss: 0.841875  [44864/60000]\n",
      "loss: 0.852602  [51264/60000]\n",
      "loss: 0.803627  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.815983 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.828425  [   64/60000]\n",
      "loss: 0.903553  [ 6464/60000]\n",
      "loss: 0.666186  [12864/60000]\n",
      "loss: 0.872598  [19264/60000]\n",
      "loss: 0.782301  [25664/60000]\n",
      "loss: 0.768442  [32064/60000]\n",
      "loss: 0.848587  [38464/60000]\n",
      "loss: 0.818004  [44864/60000]\n",
      "loss: 0.820741  [51264/60000]\n",
      "loss: 0.774144  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.785400 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.784861  [   64/60000]\n",
      "loss: 0.868907  [ 6464/60000]\n",
      "loss: 0.629467  [12864/60000]\n",
      "loss: 0.843340  [19264/60000]\n",
      "loss: 0.758513  [25664/60000]\n",
      "loss: 0.738455  [32064/60000]\n",
      "loss: 0.821005  [38464/60000]\n",
      "loss: 0.799179  [44864/60000]\n",
      "loss: 0.794928  [51264/60000]\n",
      "loss: 0.749206  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.759921 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.747906  [   64/60000]\n",
      "loss: 0.838431  [ 6464/60000]\n",
      "loss: 0.599077  [12864/60000]\n",
      "loss: 0.818950  [19264/60000]\n",
      "loss: 0.738349  [25664/60000]\n",
      "loss: 0.714411  [32064/60000]\n",
      "loss: 0.796498  [38464/60000]\n",
      "loss: 0.783396  [44864/60000]\n",
      "loss: 0.773438  [51264/60000]\n",
      "loss: 0.727674  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.737915 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.715761  [   64/60000]\n",
      "loss: 0.811147  [ 6464/60000]\n",
      "loss: 0.573247  [12864/60000]\n",
      "loss: 0.798011  [19264/60000]\n",
      "loss: 0.720917  [25664/60000]\n",
      "loss: 0.694542  [32064/60000]\n",
      "loss: 0.774073  [38464/60000]\n",
      "loss: 0.769457  [44864/60000]\n",
      "loss: 0.755116  [51264/60000]\n",
      "loss: 0.708514  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.718378 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.687150  [   64/60000]\n",
      "loss: 0.786294  [ 6464/60000]\n",
      "loss: 0.550808  [12864/60000]\n",
      "loss: 0.779667  [19264/60000]\n",
      "loss: 0.705539  [25664/60000]\n",
      "loss: 0.677835  [32064/60000]\n",
      "loss: 0.753198  [38464/60000]\n",
      "loss: 0.756696  [44864/60000]\n",
      "loss: 0.739051  [51264/60000]\n",
      "loss: 0.691013  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.700672 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.661496  [   64/60000]\n",
      "loss: 0.763481  [ 6464/60000]\n",
      "loss: 0.530909  [12864/60000]\n",
      "loss: 0.763295  [19264/60000]\n",
      "loss: 0.691617  [25664/60000]\n",
      "loss: 0.663469  [32064/60000]\n",
      "loss: 0.733472  [38464/60000]\n",
      "loss: 0.745086  [44864/60000]\n",
      "loss: 0.724639  [51264/60000]\n",
      "loss: 0.674924  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.684420 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.638363  [   64/60000]\n",
      "loss: 0.742547  [ 6464/60000]\n",
      "loss: 0.513143  [12864/60000]\n",
      "loss: 0.748306  [19264/60000]\n",
      "loss: 0.678862  [25664/60000]\n",
      "loss: 0.650872  [32064/60000]\n",
      "loss: 0.714845  [38464/60000]\n",
      "loss: 0.734341  [44864/60000]\n",
      "loss: 0.711865  [51264/60000]\n",
      "loss: 0.659990  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.669462 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.617260  [   64/60000]\n",
      "loss: 0.723297  [ 6464/60000]\n",
      "loss: 0.497256  [12864/60000]\n",
      "loss: 0.734531  [19264/60000]\n",
      "loss: 0.667363  [25664/60000]\n",
      "loss: 0.639728  [32064/60000]\n",
      "loss: 0.697441  [38464/60000]\n",
      "loss: 0.724607  [44864/60000]\n",
      "loss: 0.700726  [51264/60000]\n",
      "loss: 0.646074  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.655698 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.598168  [   64/60000]\n",
      "loss: 0.705572  [ 6464/60000]\n",
      "loss: 0.482858  [12864/60000]\n",
      "loss: 0.721807  [19264/60000]\n",
      "loss: 0.657040  [25664/60000]\n",
      "loss: 0.629934  [32064/60000]\n",
      "loss: 0.681193  [38464/60000]\n",
      "loss: 0.715958  [44864/60000]\n",
      "loss: 0.690983  [51264/60000]\n",
      "loss: 0.633191  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.643013 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.580821  [   64/60000]\n",
      "loss: 0.689326  [ 6464/60000]\n",
      "loss: 0.469713  [12864/60000]\n",
      "loss: 0.710042  [19264/60000]\n",
      "loss: 0.647700  [25664/60000]\n",
      "loss: 0.621182  [32064/60000]\n",
      "loss: 0.666062  [38464/60000]\n",
      "loss: 0.708280  [44864/60000]\n",
      "loss: 0.682513  [51264/60000]\n",
      "loss: 0.621182  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.631326 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.565021  [   64/60000]\n",
      "loss: 0.674381  [ 6464/60000]\n",
      "loss: 0.457692  [12864/60000]\n",
      "loss: 0.699072  [19264/60000]\n",
      "loss: 0.639226  [25664/60000]\n",
      "loss: 0.613330  [32064/60000]\n",
      "loss: 0.652055  [38464/60000]\n",
      "loss: 0.701610  [44864/60000]\n",
      "loss: 0.675188  [51264/60000]\n",
      "loss: 0.609902  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.620574 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.550610  [   64/60000]\n",
      "loss: 0.660642  [ 6464/60000]\n",
      "loss: 0.446736  [12864/60000]\n",
      "loss: 0.688871  [19264/60000]\n",
      "loss: 0.631361  [25664/60000]\n",
      "loss: 0.606211  [32064/60000]\n",
      "loss: 0.639071  [38464/60000]\n",
      "loss: 0.695952  [44864/60000]\n",
      "loss: 0.668880  [51264/60000]\n",
      "loss: 0.599278  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.610678 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.537349  [   64/60000]\n",
      "loss: 0.648003  [ 6464/60000]\n",
      "loss: 0.436684  [12864/60000]\n",
      "loss: 0.679322  [19264/60000]\n",
      "loss: 0.623969  [25664/60000]\n",
      "loss: 0.599674  [32064/60000]\n",
      "loss: 0.627052  [38464/60000]\n",
      "loss: 0.691310  [44864/60000]\n",
      "loss: 0.663523  [51264/60000]\n",
      "loss: 0.589204  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.601562 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.525153  [   64/60000]\n",
      "loss: 0.636389  [ 6464/60000]\n",
      "loss: 0.427425  [12864/60000]\n",
      "loss: 0.670379  [19264/60000]\n",
      "loss: 0.616935  [25664/60000]\n",
      "loss: 0.593699  [32064/60000]\n",
      "loss: 0.615926  [38464/60000]\n",
      "loss: 0.687573  [44864/60000]\n",
      "loss: 0.658979  [51264/60000]\n",
      "loss: 0.579585  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.593152 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.513873  [   64/60000]\n",
      "loss: 0.625680  [ 6464/60000]\n",
      "loss: 0.418877  [12864/60000]\n",
      "loss: 0.661905  [19264/60000]\n",
      "loss: 0.610176  [25664/60000]\n",
      "loss: 0.588140  [32064/60000]\n",
      "loss: 0.605651  [38464/60000]\n",
      "loss: 0.684570  [44864/60000]\n",
      "loss: 0.655162  [51264/60000]\n",
      "loss: 0.570383  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.585388 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.503347  [   64/60000]\n",
      "loss: 0.615768  [ 6464/60000]\n",
      "loss: 0.410960  [12864/60000]\n",
      "loss: 0.653896  [19264/60000]\n",
      "loss: 0.603593  [25664/60000]\n",
      "loss: 0.582946  [32064/60000]\n",
      "loss: 0.596178  [38464/60000]\n",
      "loss: 0.682250  [44864/60000]\n",
      "loss: 0.651923  [51264/60000]\n",
      "loss: 0.561485  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.578200 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.493504  [   64/60000]\n",
      "loss: 0.606598  [ 6464/60000]\n",
      "loss: 0.403635  [12864/60000]\n",
      "loss: 0.646332  [19264/60000]\n",
      "loss: 0.597155  [25664/60000]\n",
      "loss: 0.578045  [32064/60000]\n",
      "loss: 0.587395  [38464/60000]\n",
      "loss: 0.680610  [44864/60000]\n",
      "loss: 0.649120  [51264/60000]\n",
      "loss: 0.552958  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.571538 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.484266  [   64/60000]\n",
      "loss: 0.598105  [ 6464/60000]\n",
      "loss: 0.396782  [12864/60000]\n",
      "loss: 0.639151  [19264/60000]\n",
      "loss: 0.590780  [25664/60000]\n",
      "loss: 0.573317  [32064/60000]\n",
      "loss: 0.579328  [38464/60000]\n",
      "loss: 0.679483  [44864/60000]\n",
      "loss: 0.646705  [51264/60000]\n",
      "loss: 0.544720  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.565348 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.475616  [   64/60000]\n",
      "loss: 0.590230  [ 6464/60000]\n",
      "loss: 0.390384  [12864/60000]\n",
      "loss: 0.632321  [19264/60000]\n",
      "loss: 0.584518  [25664/60000]\n",
      "loss: 0.568639  [32064/60000]\n",
      "loss: 0.571933  [38464/60000]\n",
      "loss: 0.678843  [44864/60000]\n",
      "loss: 0.644614  [51264/60000]\n",
      "loss: 0.536745  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.559587 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.467470  [   64/60000]\n",
      "loss: 0.582967  [ 6464/60000]\n",
      "loss: 0.384388  [12864/60000]\n",
      "loss: 0.625819  [19264/60000]\n",
      "loss: 0.578361  [25664/60000]\n",
      "loss: 0.563967  [32064/60000]\n",
      "loss: 0.565136  [38464/60000]\n",
      "loss: 0.678521  [44864/60000]\n",
      "loss: 0.642757  [51264/60000]\n",
      "loss: 0.529067  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.554220 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.459818  [   64/60000]\n",
      "loss: 0.576267  [ 6464/60000]\n",
      "loss: 0.378799  [12864/60000]\n",
      "loss: 0.619594  [19264/60000]\n",
      "loss: 0.572352  [25664/60000]\n",
      "loss: 0.559424  [32064/60000]\n",
      "loss: 0.558836  [38464/60000]\n",
      "loss: 0.678497  [44864/60000]\n",
      "loss: 0.641075  [51264/60000]\n",
      "loss: 0.521647  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.549210 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.452500  [   64/60000]\n",
      "loss: 0.570087  [ 6464/60000]\n",
      "loss: 0.373585  [12864/60000]\n",
      "loss: 0.613608  [19264/60000]\n",
      "loss: 0.566410  [25664/60000]\n",
      "loss: 0.554920  [32064/60000]\n",
      "loss: 0.553007  [38464/60000]\n",
      "loss: 0.678637  [44864/60000]\n",
      "loss: 0.639476  [51264/60000]\n",
      "loss: 0.514530  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.544518 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.445595  [   64/60000]\n",
      "loss: 0.564368  [ 6464/60000]\n",
      "loss: 0.368676  [12864/60000]\n",
      "loss: 0.607897  [19264/60000]\n",
      "loss: 0.560550  [25664/60000]\n",
      "loss: 0.550459  [32064/60000]\n",
      "loss: 0.547612  [38464/60000]\n",
      "loss: 0.678906  [44864/60000]\n",
      "loss: 0.637906  [51264/60000]\n",
      "loss: 0.507670  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.540115 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.439026  [   64/60000]\n",
      "loss: 0.559063  [ 6464/60000]\n",
      "loss: 0.364061  [12864/60000]\n",
      "loss: 0.602356  [19264/60000]\n",
      "loss: 0.554788  [25664/60000]\n",
      "loss: 0.546043  [32064/60000]\n",
      "loss: 0.542565  [38464/60000]\n",
      "loss: 0.679245  [44864/60000]\n",
      "loss: 0.636389  [51264/60000]\n",
      "loss: 0.501052  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.535975 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.432819  [   64/60000]\n",
      "loss: 0.554135  [ 6464/60000]\n",
      "loss: 0.359731  [12864/60000]\n",
      "loss: 0.597021  [19264/60000]\n",
      "loss: 0.549098  [25664/60000]\n",
      "loss: 0.541722  [32064/60000]\n",
      "loss: 0.537828  [38464/60000]\n",
      "loss: 0.679625  [44864/60000]\n",
      "loss: 0.634832  [51264/60000]\n",
      "loss: 0.494719  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.532077 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.426838  [   64/60000]\n",
      "loss: 0.549527  [ 6464/60000]\n",
      "loss: 0.355593  [12864/60000]\n",
      "loss: 0.591939  [19264/60000]\n",
      "loss: 0.543489  [25664/60000]\n",
      "loss: 0.537472  [32064/60000]\n",
      "loss: 0.533386  [38464/60000]\n",
      "loss: 0.680030  [44864/60000]\n",
      "loss: 0.633327  [51264/60000]\n",
      "loss: 0.488589  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.528398 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.421107  [   64/60000]\n",
      "loss: 0.545219  [ 6464/60000]\n",
      "loss: 0.351735  [12864/60000]\n",
      "loss: 0.587054  [19264/60000]\n",
      "loss: 0.537977  [25664/60000]\n",
      "loss: 0.533279  [32064/60000]\n",
      "loss: 0.529170  [38464/60000]\n",
      "loss: 0.680389  [44864/60000]\n",
      "loss: 0.631857  [51264/60000]\n",
      "loss: 0.482688  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.524916 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.415633  [   64/60000]\n",
      "loss: 0.541175  [ 6464/60000]\n",
      "loss: 0.348140  [12864/60000]\n",
      "loss: 0.582348  [19264/60000]\n",
      "loss: 0.532567  [25664/60000]\n",
      "loss: 0.529084  [32064/60000]\n",
      "loss: 0.525180  [38464/60000]\n",
      "loss: 0.680665  [44864/60000]\n",
      "loss: 0.630325  [51264/60000]\n",
      "loss: 0.477017  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.521620 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.410338  [   64/60000]\n",
      "loss: 0.537384  [ 6464/60000]\n",
      "loss: 0.344715  [12864/60000]\n",
      "loss: 0.577769  [19264/60000]\n",
      "loss: 0.527283  [25664/60000]\n",
      "loss: 0.524955  [32064/60000]\n",
      "loss: 0.521392  [38464/60000]\n",
      "loss: 0.680814  [44864/60000]\n",
      "loss: 0.628757  [51264/60000]\n",
      "loss: 0.471571  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.518493 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.405260  [   64/60000]\n",
      "loss: 0.533839  [ 6464/60000]\n",
      "loss: 0.341456  [12864/60000]\n",
      "loss: 0.573374  [19264/60000]\n",
      "loss: 0.522083  [25664/60000]\n",
      "loss: 0.520966  [32064/60000]\n",
      "loss: 0.517807  [38464/60000]\n",
      "loss: 0.680871  [44864/60000]\n",
      "loss: 0.627188  [51264/60000]\n",
      "loss: 0.466377  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.515523 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.400348  [   64/60000]\n",
      "loss: 0.530508  [ 6464/60000]\n",
      "loss: 0.338367  [12864/60000]\n",
      "loss: 0.569122  [19264/60000]\n",
      "loss: 0.517005  [25664/60000]\n",
      "loss: 0.517021  [32064/60000]\n",
      "loss: 0.514400  [38464/60000]\n",
      "loss: 0.680788  [44864/60000]\n",
      "loss: 0.625594  [51264/60000]\n",
      "loss: 0.461476  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.512696 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 40\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7385127-d091-45f2-bb9a-acc72436e673",
   "metadata": {},
   "source": [
    "# Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa92163a-c575-4db0-b636-c315fcb64fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "23e5f20a-2674-4603-aa50-1c9101039502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\kingh/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "90bc18fe-c28f-4323-9de3-eb03b59eb3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cd01a7e-8871-4e7b-ac11-192d48a0c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"Trained_models/toy_model_to_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e62f8882-2997-4fdc-93a9-86d7a15b65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Trained_models/toy_model_to_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0082f3-b1f0-4b8e-9dcb-652403129553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
