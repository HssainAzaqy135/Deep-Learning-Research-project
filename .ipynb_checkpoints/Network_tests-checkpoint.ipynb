{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7bc87d-fe52-45e2-839c-97f7362d03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "# Extras\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "from utils import primal_sol,dual_sol,primal_projection,dual_projection,generate_vector_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635c05d-daab-4e4d-9a7e-c4cb5b1d1c66",
   "metadata": {},
   "source": [
    "### Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380ce7c3-e4e8-4b9e-8097-367a6c1b699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227755f-4d2b-4847-b12c-db814e913f63",
   "metadata": {},
   "source": [
    "### Base network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381bf497-e650-4f08-b8cc-95c11443c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Wasser_NN(nn.Module):\n",
    "    def __init__(self, n: int, d: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(OrderedDict([\n",
    "            ('Lin1', nn.Linear(2 * n * d, n * d)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('Lin2', nn.Linear(n * d, n * n + 2 * n))  # n x n matrix and two n-long vectors\n",
    "        ]))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        flattened_batch = self.flatten(batch)\n",
    "        vs1 = flattened_batch[0]\n",
    "        vs2 = flattened_batch[1]\n",
    "        x= torch.cat((vs1, vs2), dim=0)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    def predict_interval(self,batch):\n",
    "        vecs1 = batch[0]  # Shape: (n, d)\n",
    "        vecs2 = batch[1]  # Shape: (n, d)\n",
    "\n",
    "        # Compute distance matrix\n",
    "        distances_mat = compute_distance_matrix(vecs1=vecs1, vecs2=vecs2)\n",
    "        pred = self.forward(batch)\n",
    "        # Ensure that pred is the correct size for n\n",
    "        assert pred.shape[0] == n * n + 2 * n, \"Prediction tensor does not have the expected size.\"\n",
    "\n",
    "        # Reshape pred into the n x n matrix for outputs\n",
    "        out_put_mat = pred[:n*n].view(n, n)  # Shape: (n, n)\n",
    "        f = pred[n*n:n*n+n]  # Shape: (n,)\n",
    "        g = pred[n*n+n:]     # Shape: (n,)\n",
    "\n",
    "        # Compute primal and dual solutions\n",
    "        sol_prim = primal_sol(out_put_mat=out_put_mat, distances_mat=distances_mat)\n",
    "        sol_dual = dual_sol(f=f,g=g,distances_mat=distances_mat)\n",
    "        # Debug\n",
    "        # print(f\"sol_prim: {sol_prim.item()}, sol_dual: {sol_dual.item()}\")\n",
    "        \n",
    "        return (sol_dual,sol_prim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c97548-1cb3-47dd-959a-2a8a88ab7bd5",
   "metadata": {},
   "source": [
    "### Data Generation and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7186d65e-f5fc-4017-8443-32156e4ba90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batches Shape: torch.Size([800, 2, 4, 3])\n",
      "Test Batches Shape: torch.Size([200, 2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_train_test_split(count: int, dim: int, n: int, device: torch.device, coord_max: float, test_size: float = 0.2, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Generates vector batches and performs a train-test split.\n",
    "\n",
    "    Parameters:\n",
    "    - count (int): Number of batches to generate.\n",
    "    - dim (int): Dimension of each vector.\n",
    "    - n (int): Number of vectors in each batch.\n",
    "    - device (torch.device): The device on which to create the tensors.\n",
    "    - coord_max (float): The maximum absolute value for each coordinate.\n",
    "    - test_size (float): The proportion of the dataset to include in the test split.\n",
    "    - seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train_batches: A tensor with shape (train_count, 2, n, dim).\n",
    "    - test_batches: A tensor with shape (test_count, 2, n, dim).\n",
    "    \"\"\"\n",
    "    # Generate all vector batches\n",
    "    all_batches = generate_vector_batches(count, dim, n, device, coord_max, seed)\n",
    "\n",
    "    # Prepare the labels (not used directly, but can be if you add supervised learning context)\n",
    "    labels = torch.arange(count)  # Placeholder for labels, if needed\n",
    "    \n",
    "    # Perform train-test split\n",
    "    train_indices, test_indices = train_test_split(range(count), test_size=test_size, random_state=seed)\n",
    "\n",
    "    # Index into the generated batches\n",
    "    train_batches = all_batches[train_indices]\n",
    "    test_batches = all_batches[test_indices]\n",
    "\n",
    "    return train_batches, test_batches\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "count = 1000  # Total number of batches to generate\n",
    "dim = 3      # Dimension of each vector\n",
    "n = 4        # Number of vectors in each batch\n",
    "coord_max = 5.0  # Maximum coordinate value\n",
    "test_size = 0.2  # 20% of the data for testing\n",
    "\n",
    "train_batches, test_batches = generate_train_test_split(count, dim, n, device, coord_max, test_size)\n",
    "\n",
    "print(\"Train Batches Shape:\", train_batches.shape)  # Should be (train_count, 2, n, dim)\n",
    "print(\"Test Batches Shape:\", test_batches.shape)    # Should be (test_count, 2, n, dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f3792-a133-40df-8803-7db6df6165bb",
   "metadata": {},
   "source": [
    "### Loss setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475f1c6e-db3a-4b25-8eda-20d90c1f6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(vecs1, vecs2):\n",
    "    \"\"\"\n",
    "    Computes the pairwise distance matrix between two sets of vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - vecs1: Tensor of shape (n, d), where n is the number of vectors and d is the dimension of each vector.\n",
    "    - vecs2: Tensor of shape (n, d), where m is the number of vectors and d is the dimension of each vector.\n",
    "\n",
    "    Returns:\n",
    "    - distance_matrix: Tensor of shape (n, n), where each element [i, j] represents the Euclidean distance between vecs1[i] and vecs2[j].\n",
    "    \"\"\"\n",
    "    # Compute the pairwise distance matrix using torch.cdist (Euclidean distance by default)\n",
    "    distance_matrix = torch.cdist(vecs1, vecs2, p=2)\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "def make_wass_loss(n: int, d: int):\n",
    "    def wass_loss(batch, pred):\n",
    "        # Unpack vectors from the batch\n",
    "        vecs1 = batch[0]  # Shape: (n, d)\n",
    "        vecs2 = batch[1]  # Shape: (n, d)\n",
    "\n",
    "        # Compute distance matrix\n",
    "        distances_mat = compute_distance_matrix(vecs1=vecs1, vecs2=vecs2)\n",
    "\n",
    "        # Ensure that pred is the correct size for n\n",
    "        assert pred.shape[0] == n * n + 2 * n, \"Prediction tensor does not have the expected size.\"\n",
    "\n",
    "        # Reshape pred into the n x n matrix for outputs\n",
    "        out_put_mat = pred[:n*n].view(n, n)  # Shape: (n, n)\n",
    "        f = pred[n*n:n*n+n]  # Shape: (n,)\n",
    "        g = pred[n*n+n:]     # Shape: (n,)\n",
    "\n",
    "        # Compute primal and dual solutions\n",
    "        sol_prim = primal_sol(out_put_mat=out_put_mat, distances_mat=distances_mat)\n",
    "        sol_dual = dual_sol(f=f,g=g,distances_mat=distances_mat)\n",
    "        # Debug\n",
    "        # print(f\"sol_prim: {sol_prim.item()}, sol_dual: {sol_dual.item()}\")\n",
    "        \n",
    "        return sol_prim - sol_dual\n",
    "        \n",
    "    return wass_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3d5cb-8cb5-40b7-a539-5030f4b24762",
   "metadata": {},
   "source": [
    "### Train and test loops (Code currently unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19672e7e-b9c9-4036-b143-d80baec3fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, n, d):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_idx, (batch) in enumerate(dataloader):\n",
    "        vecs1, vecs2 = batch[:, 0, :, :], batch[:, 1, :, :]  # Unpack vecs1 and vecs2\n",
    "        batch_flattened = batch.view(-1, 2 * n * d)  # Flatten each sample in the batch\n",
    "\n",
    "        # Send data to device (GPU/CPU)\n",
    "        print(f\"Batch shape not flattened {batch.shape}\")\n",
    "        batch_flattened = batch_flattened.to(device)\n",
    "        print(f\"Batch shape flattened {batch_flattened.shape}\")\n",
    "        # Debug: Print input shape\n",
    "        print(f\"Input shape to model: {batch_flattened.shape}\")\n",
    "\n",
    "        # Compute model prediction\n",
    "        pred = model(batch_flattened)\n",
    "\n",
    "        # Debug: Print output shape\n",
    "        print(f\"Output shape from model: {pred.shape}\")\n",
    "\n",
    "        # Compute Wasserstein loss using vecs1 and vecs2\n",
    "        loss = loss_fn((vecs1, vecs2), pred)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss_val = loss.item()\n",
    "            current = (batch_idx + 1) * len(vecs1)\n",
    "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, _ in dataloader:\n",
    "            vecs1, vecs2 = batch[:, 0, :, :], batch[:, 1, :, :]  # Unpack vecs1 and vecs2\n",
    "            vecs1, vecs2 = vecs1.to(device), vecs2.to(device)\n",
    "\n",
    "            # Compute model prediction\n",
    "            pred = model(batch)\n",
    "\n",
    "            # Compute Wasserstein loss \n",
    "            test_loss += loss_fn((vecs1, vecs2), pred).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143759f6-0e6c-4cf7-9442-1d9cc88120ec",
   "metadata": {},
   "source": [
    "### Minor test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac90ee0f-2da7-4836-9d28-ddb763496c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base_Wasser_NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Lin1): Linear(in_features=30, out_features=15, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (Lin2): Linear(in_features=15, out_features=35, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Average Test Loss: 10.3777\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Average Test Loss: -1.6269\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Average Test Loss: -4.6564\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Average Test Loss: -5.4944\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Average Test Loss: -5.9728\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Average Test Loss: -6.1677\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Average Test Loss: -6.3844\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Average Test Loss: -6.4752\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Average Test Loss: -6.5852\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Average Test Loss: -6.6677\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_batches, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model on the provided training batches.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network model to train.\n",
    "    - train_batches: Tensor containing training batches of shape (count, 2, n, d).\n",
    "    - loss_fn: The loss function.\n",
    "    - optimizer: The optimizer for model parameters.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    for i, batch in enumerate(train_batches):  # Iterate over all training batches\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(batch)  # Model output\n",
    "\n",
    "        \n",
    "        # Assuming that `batch` contains two tensors for the Wasserstein loss\n",
    "        loss = loss_fn(batch, logits)  # Compute the loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_batches, loss_fn):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided testing batches.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The neural network model to evaluate.\n",
    "    - test_batches: Tensor containing testing batches of shape (count, 2, n, d).\n",
    "    - loss_fn: The loss function.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_loss: Average loss on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_batches):  # Iterate over all test batches\n",
    "            # Flatten the batch to the expected input shape\n",
    "            batch_flattened = batch.view(batch.size(0), -1)  # Shape: (batch_size, 2 * n * d)\n",
    "\n",
    "            logits = model(batch_flattened)  # Model output\n",
    "            \n",
    "            loss = loss_fn(batch, logits)  # Compute the loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_batches)\n",
    "    return avg_loss\n",
    "\n",
    "# Define the parameters\n",
    "count = 100  # Number of batches\n",
    "d = 3         # Dimension of each vector\n",
    "n = 5         # Number of vectors in each batch\n",
    "seed = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "coords_max = 10\n",
    "# Generate train and test batches\n",
    "train_batches, test_batches = generate_train_test_split(\n",
    "    count=count,\n",
    "    dim=d,\n",
    "    n=n,\n",
    "    device=device,\n",
    "    coord_max=coords_max,\n",
    "    test_size=0.2,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Create Loss function\n",
    "loss_fn = make_wass_loss(n=n, d=d)\n",
    "\n",
    "# Model instance\n",
    "model = Base_Wasser_NN(n=n, d=d).to(device)\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "print(model)\n",
    "# Training and Testing Loop\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train_model(model, train_batches, loss_fn, optimizer)  # Training\n",
    "    avg_loss = evaluate_model(model, test_batches, loss_fn)  # Testing\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca6eb43-e218-42e6-a374-63e25c85625d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.2568,  2.3878, -2.7707,  0.4141, -6.4845, -6.5412,  2.2804,  1.5064,\n",
       "         -6.0107,  8.4472, -5.3848,  1.9260, -2.5767,  1.2138, -1.9201],\n",
       "        [ 5.8379,  6.0114, -1.8126, -8.5362, -9.0247, -9.0821,  7.6054, -6.3591,\n",
       "          5.5840,  7.7015, -2.9452,  5.3066,  1.9822,  0.9520,  4.6147]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Flatten()(test_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c32854e-d117-405b-9c69-5774f7dd92b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(33.1571, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(25.2156, device='cuda:0', grad_fn=<SqrtBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_interval(test_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e4a97-8054-4b65-b8b8-6273be154376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
