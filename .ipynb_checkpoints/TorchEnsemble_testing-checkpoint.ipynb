{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15bac7d2-14d0-4c4d-85d2-cd536f776bec",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d0d813-f1b5-489b-b667-84c583211908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchensemble import VotingClassifier\n",
    "# Extras\n",
    "import time\n",
    "# Torch Ensemble imports\n",
    "from torchensemble import VotingClassifier  # voting is a classic ensemble strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baacb393-ebf0-4e22-9b90-759653e1fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "# Set training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "epochs = 15\n",
    "\n",
    "base_estimator = nn.Sequential(\n",
    "    nn.Flatten(),            # Flatten the 28x28 input images to a vector of 784\n",
    "    nn.Linear(28 * 28, 256), # First fully connected layer\n",
    "    nn.ReLU(),               # Activation function\n",
    "    nn.Linear(256, 128),     # Second fully connected layer\n",
    "    nn.ReLU(),               # Activation function\n",
    "    nn.Linear(128, 10)       # Output layer for 10 classes\n",
    ")\n",
    "# If using GPU, transfer data to the GPU inside fit() and evaluate() loops\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set the base estimator to the device\n",
    "base_estimator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e61f523-a77f-42ce-be1a-b12d7fe668db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize using MNIST mean and std\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f67196-2288-49f8-ab46-6b9b4ec8571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimator=base_estimator,  # here is your deep learning model\n",
    "    n_estimators=5,           # number of base estimators\n",
    "    cuda=torch.cuda.is_available()  # Enable GPU if available\n",
    ")\n",
    "\n",
    "# Set the criterion\n",
    "criterion = nn.CrossEntropyLoss()  # training objective\n",
    "ensemble.set_criterion(criterion)\n",
    "\n",
    "# Set the optimizer\n",
    "ensemble.set_optimizer(\n",
    "    \"Adam\",                       # type of parameter optimizer\n",
    "    lr=learning_rate,             # learning rate of parameter optimizer\n",
    "    weight_decay=weight_decay,    # weight decay of parameter optimizer\n",
    ")\n",
    "\n",
    "# Set the learning rate scheduler\n",
    "ensemble.set_scheduler(\n",
    "    \"CosineAnnealingLR\",          # type of learning rate scheduler\n",
    "    T_max=epochs,                 # number of epochs for decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ffce6-ae45-43eb-b7d2-3ce50c015afc",
   "metadata": {},
   "source": [
    "# Train the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ba9f07-21eb-4286-ada2-775a5bf7f374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 2.31229 | Correct: 3/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 0.18821 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 0.24857 | Correct: 59/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 0.18057 | Correct: 58/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 0.22935 | Correct: 59/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 0.30720 | Correct: 58/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 0.02711 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 0.19426 | Correct: 58/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 800 | Loss: 0.14643 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 000 | Batch: 900 | Loss: 0.13720 | Correct: 61/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 000 | Loss: 2.32213 | Correct: 3/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 100 | Loss: 0.32895 | Correct: 59/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 200 | Loss: 0.41220 | Correct: 58/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 300 | Loss: 0.16070 | Correct: 61/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 400 | Loss: 0.28158 | Correct: 58/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 500 | Loss: 0.27331 | Correct: 58/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 600 | Loss: 0.23453 | Correct: 59/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 700 | Loss: 0.12977 | Correct: 60/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 800 | Loss: 0.09802 | Correct: 62/64\n",
      "Estimator: 001 | Epoch: 000 | Batch: 900 | Loss: 0.24600 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 000 | Loss: 2.32277 | Correct: 3/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 100 | Loss: 0.30255 | Correct: 56/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 200 | Loss: 0.18737 | Correct: 61/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 300 | Loss: 0.30922 | Correct: 58/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 400 | Loss: 0.13578 | Correct: 59/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 500 | Loss: 0.24425 | Correct: 61/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 600 | Loss: 0.14769 | Correct: 60/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 700 | Loss: 0.29428 | Correct: 57/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 800 | Loss: 0.05454 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 000 | Batch: 900 | Loss: 0.10587 | Correct: 61/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 000 | Loss: 2.33454 | Correct: 1/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 100 | Loss: 0.31804 | Correct: 60/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 200 | Loss: 0.24765 | Correct: 60/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 300 | Loss: 0.24057 | Correct: 61/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 400 | Loss: 0.12673 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 500 | Loss: 0.04440 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 600 | Loss: 0.24915 | Correct: 59/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 700 | Loss: 0.21424 | Correct: 59/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 800 | Loss: 0.19830 | Correct: 59/64\n",
      "Estimator: 003 | Epoch: 000 | Batch: 900 | Loss: 0.01724 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 000 | Loss: 2.32782 | Correct: 2/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 100 | Loss: 0.41321 | Correct: 55/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 200 | Loss: 0.27146 | Correct: 58/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 300 | Loss: 0.18517 | Correct: 59/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 400 | Loss: 0.20418 | Correct: 60/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 500 | Loss: 0.09342 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 600 | Loss: 0.16207 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 700 | Loss: 0.08325 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 800 | Loss: 0.14749 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 000 | Batch: 900 | Loss: 0.10494 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 000 | Loss: 0.05917 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 100 | Loss: 0.05032 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 200 | Loss: 0.04730 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 300 | Loss: 0.14213 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 400 | Loss: 0.04853 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 500 | Loss: 0.10614 | Correct: 61/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 600 | Loss: 0.16357 | Correct: 61/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 700 | Loss: 0.27002 | Correct: 60/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 800 | Loss: 0.09077 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 001 | Batch: 900 | Loss: 0.10778 | Correct: 60/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 000 | Loss: 0.13094 | Correct: 62/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 100 | Loss: 0.05961 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 200 | Loss: 0.22595 | Correct: 59/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 300 | Loss: 0.05448 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 400 | Loss: 0.16999 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 500 | Loss: 0.02261 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 600 | Loss: 0.03883 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 700 | Loss: 0.16170 | Correct: 60/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 800 | Loss: 0.13202 | Correct: 61/64\n",
      "Estimator: 001 | Epoch: 001 | Batch: 900 | Loss: 0.09347 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 000 | Loss: 0.11282 | Correct: 61/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 100 | Loss: 0.06542 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 200 | Loss: 0.06920 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 300 | Loss: 0.02645 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 400 | Loss: 0.04643 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 500 | Loss: 0.05873 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 600 | Loss: 0.13076 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 700 | Loss: 0.16070 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 800 | Loss: 0.05522 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 001 | Batch: 900 | Loss: 0.18793 | Correct: 61/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 000 | Loss: 0.15718 | Correct: 60/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 100 | Loss: 0.04468 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 200 | Loss: 0.12738 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 300 | Loss: 0.12140 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 400 | Loss: 0.11664 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 500 | Loss: 0.17695 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 600 | Loss: 0.05686 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 700 | Loss: 0.03494 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 800 | Loss: 0.18089 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 001 | Batch: 900 | Loss: 0.13225 | Correct: 60/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 000 | Loss: 0.08295 | Correct: 61/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 100 | Loss: 0.12097 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 200 | Loss: 0.12878 | Correct: 60/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 300 | Loss: 0.06484 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 400 | Loss: 0.08554 | Correct: 61/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 500 | Loss: 0.08090 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 600 | Loss: 0.04768 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 700 | Loss: 0.14920 | Correct: 60/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 800 | Loss: 0.28021 | Correct: 58/64\n",
      "Estimator: 004 | Epoch: 001 | Batch: 900 | Loss: 0.03252 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 000 | Loss: 0.03161 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 100 | Loss: 0.07165 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 200 | Loss: 0.02045 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 300 | Loss: 0.00571 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 400 | Loss: 0.02367 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 500 | Loss: 0.03703 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 600 | Loss: 0.14010 | Correct: 61/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 700 | Loss: 0.03081 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 800 | Loss: 0.01619 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 002 | Batch: 900 | Loss: 0.04827 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 000 | Loss: 0.06934 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 100 | Loss: 0.01946 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 200 | Loss: 0.12126 | Correct: 62/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 300 | Loss: 0.01500 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 400 | Loss: 0.06894 | Correct: 62/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 500 | Loss: 0.05535 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 600 | Loss: 0.02155 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 700 | Loss: 0.01669 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 800 | Loss: 0.03075 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 002 | Batch: 900 | Loss: 0.05254 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 000 | Loss: 0.07296 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 100 | Loss: 0.07342 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 200 | Loss: 0.11590 | Correct: 61/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 300 | Loss: 0.07251 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 400 | Loss: 0.09124 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 500 | Loss: 0.01428 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 600 | Loss: 0.04207 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 700 | Loss: 0.09015 | Correct: 61/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 800 | Loss: 0.07100 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 002 | Batch: 900 | Loss: 0.01250 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 000 | Loss: 0.08226 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 100 | Loss: 0.06028 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 200 | Loss: 0.05681 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 300 | Loss: 0.05554 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 400 | Loss: 0.08245 | Correct: 61/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 500 | Loss: 0.02959 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 600 | Loss: 0.22266 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 700 | Loss: 0.01698 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 800 | Loss: 0.14265 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 002 | Batch: 900 | Loss: 0.05430 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 000 | Loss: 0.01677 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 100 | Loss: 0.05625 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 200 | Loss: 0.03245 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 300 | Loss: 0.06206 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 400 | Loss: 0.11198 | Correct: 61/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 500 | Loss: 0.02002 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 600 | Loss: 0.01766 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 700 | Loss: 0.09379 | Correct: 61/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 800 | Loss: 0.05116 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 002 | Batch: 900 | Loss: 0.05741 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 000 | Loss: 0.15092 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 100 | Loss: 0.01002 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 200 | Loss: 0.04197 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 300 | Loss: 0.01476 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 400 | Loss: 0.02007 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 500 | Loss: 0.07719 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 600 | Loss: 0.04442 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 700 | Loss: 0.10283 | Correct: 61/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 800 | Loss: 0.17472 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 003 | Batch: 900 | Loss: 0.01218 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 000 | Loss: 0.03963 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 100 | Loss: 0.09521 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 200 | Loss: 0.04390 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 300 | Loss: 0.00916 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 400 | Loss: 0.03559 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 500 | Loss: 0.01346 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 600 | Loss: 0.05798 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 700 | Loss: 0.01035 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 800 | Loss: 0.02077 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 003 | Batch: 900 | Loss: 0.02108 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 000 | Loss: 0.07537 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 100 | Loss: 0.01754 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 200 | Loss: 0.01042 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 300 | Loss: 0.03170 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 400 | Loss: 0.03561 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 500 | Loss: 0.05033 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 600 | Loss: 0.18769 | Correct: 59/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 700 | Loss: 0.01750 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 800 | Loss: 0.02332 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 003 | Batch: 900 | Loss: 0.09772 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 000 | Loss: 0.03021 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 100 | Loss: 0.07982 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 200 | Loss: 0.03071 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 300 | Loss: 0.01972 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 400 | Loss: 0.02960 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 500 | Loss: 0.01712 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 600 | Loss: 0.15809 | Correct: 61/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 700 | Loss: 0.05937 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 800 | Loss: 0.01248 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 003 | Batch: 900 | Loss: 0.00783 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 000 | Loss: 0.02479 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 100 | Loss: 0.01764 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 200 | Loss: 0.14083 | Correct: 61/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 300 | Loss: 0.01645 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 400 | Loss: 0.01813 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 500 | Loss: 0.07268 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 600 | Loss: 0.02785 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 700 | Loss: 0.07303 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 800 | Loss: 0.04628 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 003 | Batch: 900 | Loss: 0.11753 | Correct: 61/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 000 | Loss: 0.00514 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 100 | Loss: 0.02587 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 200 | Loss: 0.01594 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 300 | Loss: 0.03253 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 400 | Loss: 0.05501 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 500 | Loss: 0.03055 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 600 | Loss: 0.03468 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 700 | Loss: 0.07859 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 800 | Loss: 0.01076 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 004 | Batch: 900 | Loss: 0.02515 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 000 | Loss: 0.01814 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 100 | Loss: 0.07304 | Correct: 62/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 200 | Loss: 0.00265 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 300 | Loss: 0.03144 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 400 | Loss: 0.01498 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 500 | Loss: 0.01817 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 600 | Loss: 0.01448 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 700 | Loss: 0.09585 | Correct: 61/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 800 | Loss: 0.06760 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 004 | Batch: 900 | Loss: 0.10151 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 000 | Loss: 0.06367 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 100 | Loss: 0.01613 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 200 | Loss: 0.02734 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 300 | Loss: 0.01669 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 400 | Loss: 0.03532 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 500 | Loss: 0.06890 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 600 | Loss: 0.01169 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 700 | Loss: 0.01293 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 800 | Loss: 0.04966 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 004 | Batch: 900 | Loss: 0.06527 | Correct: 61/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 000 | Loss: 0.03723 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 100 | Loss: 0.05545 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 200 | Loss: 0.00993 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 300 | Loss: 0.00809 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 400 | Loss: 0.00645 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 500 | Loss: 0.06975 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 600 | Loss: 0.00755 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 700 | Loss: 0.02951 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 800 | Loss: 0.08372 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 004 | Batch: 900 | Loss: 0.04549 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 000 | Loss: 0.02086 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 100 | Loss: 0.02338 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 200 | Loss: 0.00600 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 300 | Loss: 0.07845 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 400 | Loss: 0.03770 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 500 | Loss: 0.02661 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 600 | Loss: 0.01179 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 700 | Loss: 0.02063 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 800 | Loss: 0.04090 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 004 | Batch: 900 | Loss: 0.05390 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 000 | Loss: 0.00357 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 100 | Loss: 0.06173 | Correct: 61/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 200 | Loss: 0.00222 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 300 | Loss: 0.01411 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 400 | Loss: 0.01857 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 500 | Loss: 0.00649 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 600 | Loss: 0.11799 | Correct: 62/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 700 | Loss: 0.02201 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 800 | Loss: 0.01128 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 005 | Batch: 900 | Loss: 0.00690 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 000 | Loss: 0.01931 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 100 | Loss: 0.01358 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 200 | Loss: 0.01355 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 300 | Loss: 0.00946 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 400 | Loss: 0.01024 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 500 | Loss: 0.06690 | Correct: 62/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 600 | Loss: 0.04324 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 700 | Loss: 0.04287 | Correct: 62/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 800 | Loss: 0.03839 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 005 | Batch: 900 | Loss: 0.06075 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 000 | Loss: 0.04611 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 100 | Loss: 0.04911 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 200 | Loss: 0.02184 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 300 | Loss: 0.03578 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 400 | Loss: 0.00796 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 500 | Loss: 0.01707 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 600 | Loss: 0.00517 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 700 | Loss: 0.00615 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 800 | Loss: 0.01346 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 005 | Batch: 900 | Loss: 0.05725 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 000 | Loss: 0.00512 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 100 | Loss: 0.00632 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 200 | Loss: 0.01290 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 300 | Loss: 0.04470 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 400 | Loss: 0.01085 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 500 | Loss: 0.00607 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 600 | Loss: 0.01298 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 700 | Loss: 0.04989 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 800 | Loss: 0.01386 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 005 | Batch: 900 | Loss: 0.05433 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 000 | Loss: 0.06115 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 100 | Loss: 0.00246 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 200 | Loss: 0.00208 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 300 | Loss: 0.02428 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 400 | Loss: 0.02450 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 500 | Loss: 0.02519 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 600 | Loss: 0.00772 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 700 | Loss: 0.05185 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 800 | Loss: 0.03717 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 005 | Batch: 900 | Loss: 0.15064 | Correct: 61/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 000 | Loss: 0.00718 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 100 | Loss: 0.02997 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 200 | Loss: 0.01592 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 300 | Loss: 0.02619 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 400 | Loss: 0.00959 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 500 | Loss: 0.03573 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 600 | Loss: 0.00429 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 700 | Loss: 0.00425 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 800 | Loss: 0.01010 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 006 | Batch: 900 | Loss: 0.03173 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 000 | Loss: 0.00694 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 100 | Loss: 0.00551 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 200 | Loss: 0.00347 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 300 | Loss: 0.00366 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 400 | Loss: 0.01971 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 500 | Loss: 0.02956 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 600 | Loss: 0.00814 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 700 | Loss: 0.00629 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 800 | Loss: 0.00543 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 006 | Batch: 900 | Loss: 0.01827 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 000 | Loss: 0.00716 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 100 | Loss: 0.01719 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 200 | Loss: 0.00088 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 300 | Loss: 0.03178 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 400 | Loss: 0.00873 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 500 | Loss: 0.00549 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 600 | Loss: 0.09345 | Correct: 61/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 700 | Loss: 0.08614 | Correct: 62/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 800 | Loss: 0.00761 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 006 | Batch: 900 | Loss: 0.09928 | Correct: 61/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 000 | Loss: 0.04127 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 100 | Loss: 0.00378 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 200 | Loss: 0.14049 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 300 | Loss: 0.00621 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 400 | Loss: 0.00082 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 500 | Loss: 0.01681 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 600 | Loss: 0.02085 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 700 | Loss: 0.03561 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 800 | Loss: 0.03661 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 006 | Batch: 900 | Loss: 0.00609 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 000 | Loss: 0.00938 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 100 | Loss: 0.00379 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 200 | Loss: 0.00209 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 300 | Loss: 0.05030 | Correct: 62/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 400 | Loss: 0.00198 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 500 | Loss: 0.01085 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 600 | Loss: 0.02668 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 700 | Loss: 0.00746 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 800 | Loss: 0.01295 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 006 | Batch: 900 | Loss: 0.00037 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 000 | Loss: 0.03502 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 100 | Loss: 0.00746 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 200 | Loss: 0.00057 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 300 | Loss: 0.00493 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 400 | Loss: 0.03758 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 500 | Loss: 0.00536 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 600 | Loss: 0.00118 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 700 | Loss: 0.00987 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 800 | Loss: 0.00248 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 007 | Batch: 900 | Loss: 0.00137 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 000 | Loss: 0.05910 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 100 | Loss: 0.00047 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 200 | Loss: 0.02106 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 300 | Loss: 0.00183 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 400 | Loss: 0.00671 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 500 | Loss: 0.00388 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 600 | Loss: 0.00331 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 700 | Loss: 0.04098 | Correct: 62/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 800 | Loss: 0.00462 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 007 | Batch: 900 | Loss: 0.00076 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 000 | Loss: 0.01056 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 100 | Loss: 0.04956 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 200 | Loss: 0.00107 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 300 | Loss: 0.00260 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 400 | Loss: 0.00535 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 500 | Loss: 0.01203 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 600 | Loss: 0.00170 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 700 | Loss: 0.00054 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 800 | Loss: 0.00185 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 007 | Batch: 900 | Loss: 0.01429 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 000 | Loss: 0.00754 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 100 | Loss: 0.04129 | Correct: 62/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 200 | Loss: 0.00054 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 300 | Loss: 0.00034 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 400 | Loss: 0.00132 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 500 | Loss: 0.00528 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 600 | Loss: 0.04081 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 700 | Loss: 0.00271 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 800 | Loss: 0.00069 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 007 | Batch: 900 | Loss: 0.02404 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 000 | Loss: 0.02001 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 100 | Loss: 0.00971 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 200 | Loss: 0.00082 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 300 | Loss: 0.00865 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 400 | Loss: 0.00651 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 500 | Loss: 0.00110 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 600 | Loss: 0.00557 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 700 | Loss: 0.00233 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 800 | Loss: 0.01708 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 007 | Batch: 900 | Loss: 0.02383 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 000 | Loss: 0.01162 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 100 | Loss: 0.00316 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 200 | Loss: 0.00046 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 300 | Loss: 0.00056 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 400 | Loss: 0.00181 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 500 | Loss: 0.01651 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 600 | Loss: 0.00193 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 700 | Loss: 0.01202 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 800 | Loss: 0.00450 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 008 | Batch: 900 | Loss: 0.00230 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 000 | Loss: 0.02555 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 100 | Loss: 0.00225 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 200 | Loss: 0.00883 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 300 | Loss: 0.00216 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 400 | Loss: 0.00677 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 500 | Loss: 0.00216 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 600 | Loss: 0.05898 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 700 | Loss: 0.04286 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 800 | Loss: 0.01125 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 008 | Batch: 900 | Loss: 0.00972 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 000 | Loss: 0.00396 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 100 | Loss: 0.01666 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 200 | Loss: 0.00041 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 300 | Loss: 0.00047 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 400 | Loss: 0.00529 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 500 | Loss: 0.00029 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 600 | Loss: 0.00097 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 700 | Loss: 0.03558 | Correct: 63/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 800 | Loss: 0.00897 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 008 | Batch: 900 | Loss: 0.06507 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 000 | Loss: 0.00343 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 100 | Loss: 0.00244 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 200 | Loss: 0.00511 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 300 | Loss: 0.00027 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 400 | Loss: 0.00078 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 500 | Loss: 0.00041 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 600 | Loss: 0.00411 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 700 | Loss: 0.01369 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 800 | Loss: 0.06823 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 008 | Batch: 900 | Loss: 0.00272 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 000 | Loss: 0.01108 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 100 | Loss: 0.01521 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 200 | Loss: 0.00131 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 300 | Loss: 0.01658 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 400 | Loss: 0.00006 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 500 | Loss: 0.00134 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 600 | Loss: 0.00253 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 700 | Loss: 0.04935 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 800 | Loss: 0.00481 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 008 | Batch: 900 | Loss: 0.00072 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 000 | Loss: 0.00445 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 100 | Loss: 0.00514 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 200 | Loss: 0.00061 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 300 | Loss: 0.00538 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 400 | Loss: 0.00721 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 500 | Loss: 0.00159 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 600 | Loss: 0.00136 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 700 | Loss: 0.00095 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 800 | Loss: 0.00310 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 009 | Batch: 900 | Loss: 0.00502 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 000 | Loss: 0.00977 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 100 | Loss: 0.00560 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 200 | Loss: 0.00532 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 300 | Loss: 0.00533 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 400 | Loss: 0.01135 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 500 | Loss: 0.00600 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 600 | Loss: 0.00228 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 700 | Loss: 0.00514 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 800 | Loss: 0.00189 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 009 | Batch: 900 | Loss: 0.00494 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 000 | Loss: 0.01223 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 100 | Loss: 0.00230 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 200 | Loss: 0.00178 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 300 | Loss: 0.00969 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 400 | Loss: 0.01247 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 500 | Loss: 0.00083 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 600 | Loss: 0.00123 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 700 | Loss: 0.00412 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 800 | Loss: 0.00894 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 009 | Batch: 900 | Loss: 0.01014 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 000 | Loss: 0.01118 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 100 | Loss: 0.04114 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 200 | Loss: 0.00319 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 300 | Loss: 0.00036 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 400 | Loss: 0.00315 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 500 | Loss: 0.00117 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 600 | Loss: 0.00098 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 700 | Loss: 0.06101 | Correct: 63/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 800 | Loss: 0.00051 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 009 | Batch: 900 | Loss: 0.00094 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 000 | Loss: 0.00215 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 100 | Loss: 0.00041 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 200 | Loss: 0.01057 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 300 | Loss: 0.00741 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 400 | Loss: 0.00250 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 500 | Loss: 0.01065 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 600 | Loss: 0.00080 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 700 | Loss: 0.01995 | Correct: 63/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 800 | Loss: 0.00033 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 009 | Batch: 900 | Loss: 0.00395 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 000 | Loss: 0.00010 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 100 | Loss: 0.00799 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 200 | Loss: 0.00186 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 300 | Loss: 0.00030 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 400 | Loss: 0.00026 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 500 | Loss: 0.00119 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 600 | Loss: 0.01892 | Correct: 63/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 700 | Loss: 0.00028 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 800 | Loss: 0.00113 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 010 | Batch: 900 | Loss: 0.01399 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 000 | Loss: 0.03896 | Correct: 63/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 100 | Loss: 0.00405 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 200 | Loss: 0.00022 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 300 | Loss: 0.00262 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 400 | Loss: 0.00319 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 500 | Loss: 0.00047 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 600 | Loss: 0.00194 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 700 | Loss: 0.00056 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 800 | Loss: 0.00066 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 010 | Batch: 900 | Loss: 0.00127 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 000 | Loss: 0.00079 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 100 | Loss: 0.00248 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 200 | Loss: 0.00201 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 300 | Loss: 0.00359 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 400 | Loss: 0.00012 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 500 | Loss: 0.00224 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 600 | Loss: 0.00322 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 700 | Loss: 0.00144 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 800 | Loss: 0.00225 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 010 | Batch: 900 | Loss: 0.00085 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 000 | Loss: 0.00439 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 100 | Loss: 0.00288 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 200 | Loss: 0.00223 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 300 | Loss: 0.00174 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 400 | Loss: 0.00109 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 500 | Loss: 0.00028 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 600 | Loss: 0.00150 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 700 | Loss: 0.00416 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 800 | Loss: 0.00090 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 010 | Batch: 900 | Loss: 0.00660 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 000 | Loss: 0.00428 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 100 | Loss: 0.00146 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 200 | Loss: 0.00048 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 300 | Loss: 0.01137 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 400 | Loss: 0.00115 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 500 | Loss: 0.00047 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 600 | Loss: 0.00042 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 700 | Loss: 0.00659 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 800 | Loss: 0.00050 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 010 | Batch: 900 | Loss: 0.00184 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 000 | Loss: 0.00241 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 100 | Loss: 0.00045 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 200 | Loss: 0.00140 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 300 | Loss: 0.00012 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 400 | Loss: 0.01446 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 500 | Loss: 0.00162 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 600 | Loss: 0.00012 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 700 | Loss: 0.00040 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 800 | Loss: 0.00075 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 011 | Batch: 900 | Loss: 0.00021 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 000 | Loss: 0.00035 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 100 | Loss: 0.00336 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 200 | Loss: 0.00157 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 300 | Loss: 0.00065 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 400 | Loss: 0.00117 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 500 | Loss: 0.00093 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 600 | Loss: 0.00097 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 700 | Loss: 0.00006 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 800 | Loss: 0.00080 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 011 | Batch: 900 | Loss: 0.00090 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 000 | Loss: 0.00213 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 100 | Loss: 0.00606 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 200 | Loss: 0.00049 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 300 | Loss: 0.00104 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 400 | Loss: 0.00114 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 500 | Loss: 0.00270 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 600 | Loss: 0.00066 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 700 | Loss: 0.00078 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 800 | Loss: 0.00048 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 011 | Batch: 900 | Loss: 0.00174 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 000 | Loss: 0.00223 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 100 | Loss: 0.00209 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 200 | Loss: 0.00164 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 300 | Loss: 0.00100 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 400 | Loss: 0.00141 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 500 | Loss: 0.00198 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 600 | Loss: 0.00041 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 700 | Loss: 0.00021 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 800 | Loss: 0.00026 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 011 | Batch: 900 | Loss: 0.00148 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 000 | Loss: 0.00117 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 100 | Loss: 0.00105 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 200 | Loss: 0.00086 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 300 | Loss: 0.00201 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 400 | Loss: 0.00163 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 500 | Loss: 0.00137 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 600 | Loss: 0.00041 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 700 | Loss: 0.00203 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 800 | Loss: 0.00020 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 011 | Batch: 900 | Loss: 0.00041 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 000 | Loss: 0.00296 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 100 | Loss: 0.00164 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 200 | Loss: 0.00267 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 300 | Loss: 0.00199 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 400 | Loss: 0.00017 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 500 | Loss: 0.00320 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 600 | Loss: 0.00174 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 700 | Loss: 0.00485 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 800 | Loss: 0.00088 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 012 | Batch: 900 | Loss: 0.00234 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 000 | Loss: 0.00074 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 100 | Loss: 0.00174 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 200 | Loss: 0.00171 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 300 | Loss: 0.00219 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 400 | Loss: 0.00304 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 500 | Loss: 0.00137 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 600 | Loss: 0.00072 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 700 | Loss: 0.00014 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 800 | Loss: 0.00082 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 012 | Batch: 900 | Loss: 0.00016 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 000 | Loss: 0.00030 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 100 | Loss: 0.00106 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 200 | Loss: 0.00037 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 300 | Loss: 0.00113 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 400 | Loss: 0.00312 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 500 | Loss: 0.00022 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 600 | Loss: 0.00045 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 700 | Loss: 0.00167 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 800 | Loss: 0.00114 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 012 | Batch: 900 | Loss: 0.00297 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 000 | Loss: 0.00084 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 100 | Loss: 0.00071 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 200 | Loss: 0.00045 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 300 | Loss: 0.00095 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 400 | Loss: 0.00037 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 500 | Loss: 0.00131 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 600 | Loss: 0.00069 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 700 | Loss: 0.00018 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 800 | Loss: 0.00159 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 012 | Batch: 900 | Loss: 0.00169 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 000 | Loss: 0.00219 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 100 | Loss: 0.00341 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 200 | Loss: 0.00026 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 300 | Loss: 0.00015 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 400 | Loss: 0.00205 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 500 | Loss: 0.00023 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 600 | Loss: 0.00094 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 700 | Loss: 0.00021 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 800 | Loss: 0.00072 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 012 | Batch: 900 | Loss: 0.00219 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 000 | Loss: 0.00046 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 100 | Loss: 0.00072 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 200 | Loss: 0.00062 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 300 | Loss: 0.00099 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 400 | Loss: 0.00039 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 500 | Loss: 0.00130 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 600 | Loss: 0.00056 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 700 | Loss: 0.00019 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 800 | Loss: 0.00077 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 013 | Batch: 900 | Loss: 0.00174 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 000 | Loss: 0.00182 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 100 | Loss: 0.00068 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 200 | Loss: 0.00039 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 300 | Loss: 0.00214 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 400 | Loss: 0.00048 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 500 | Loss: 0.00039 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 600 | Loss: 0.00054 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 700 | Loss: 0.00041 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 800 | Loss: 0.00309 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 013 | Batch: 900 | Loss: 0.00033 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 000 | Loss: 0.00010 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 100 | Loss: 0.00077 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 200 | Loss: 0.00064 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 300 | Loss: 0.00519 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 400 | Loss: 0.00135 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 500 | Loss: 0.00044 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 600 | Loss: 0.00147 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 700 | Loss: 0.00016 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 800 | Loss: 0.00118 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 013 | Batch: 900 | Loss: 0.00127 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 000 | Loss: 0.00095 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 100 | Loss: 0.00069 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 200 | Loss: 0.00120 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 300 | Loss: 0.00107 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 400 | Loss: 0.00051 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 500 | Loss: 0.00031 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 600 | Loss: 0.00059 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 700 | Loss: 0.00038 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 800 | Loss: 0.00028 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 013 | Batch: 900 | Loss: 0.00102 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 000 | Loss: 0.00029 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 100 | Loss: 0.00058 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 200 | Loss: 0.00064 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 300 | Loss: 0.00088 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 400 | Loss: 0.00174 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 500 | Loss: 0.00086 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 600 | Loss: 0.00062 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 700 | Loss: 0.00088 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 800 | Loss: 0.00065 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 013 | Batch: 900 | Loss: 0.00122 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 000 | Loss: 0.00028 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 100 | Loss: 0.00149 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 200 | Loss: 0.00049 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 300 | Loss: 0.00085 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 400 | Loss: 0.00072 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 500 | Loss: 0.00064 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 600 | Loss: 0.00090 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 700 | Loss: 0.00009 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 800 | Loss: 0.00200 | Correct: 64/64\n",
      "Estimator: 000 | Epoch: 014 | Batch: 900 | Loss: 0.00142 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 000 | Loss: 0.00052 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 100 | Loss: 0.00078 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 200 | Loss: 0.00014 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 300 | Loss: 0.00262 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 400 | Loss: 0.00014 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 500 | Loss: 0.00025 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 600 | Loss: 0.00098 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 700 | Loss: 0.00114 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 800 | Loss: 0.00027 | Correct: 64/64\n",
      "Estimator: 001 | Epoch: 014 | Batch: 900 | Loss: 0.00235 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 000 | Loss: 0.00037 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 100 | Loss: 0.00032 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 200 | Loss: 0.00124 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 300 | Loss: 0.00246 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 400 | Loss: 0.00153 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 500 | Loss: 0.00013 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 600 | Loss: 0.00060 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 700 | Loss: 0.00164 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 800 | Loss: 0.00105 | Correct: 64/64\n",
      "Estimator: 002 | Epoch: 014 | Batch: 900 | Loss: 0.00058 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 000 | Loss: 0.00193 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 100 | Loss: 0.00061 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 200 | Loss: 0.00055 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 300 | Loss: 0.00051 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 400 | Loss: 0.00044 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 500 | Loss: 0.00021 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 600 | Loss: 0.00041 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 700 | Loss: 0.00089 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 800 | Loss: 0.00052 | Correct: 64/64\n",
      "Estimator: 003 | Epoch: 014 | Batch: 900 | Loss: 0.00035 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 000 | Loss: 0.00009 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 100 | Loss: 0.00077 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 200 | Loss: 0.00046 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 300 | Loss: 0.00100 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 400 | Loss: 0.00091 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 500 | Loss: 0.00017 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 600 | Loss: 0.00229 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 700 | Loss: 0.00154 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 800 | Loss: 0.00043 | Correct: 64/64\n",
      "Estimator: 004 | Epoch: 014 | Batch: 900 | Loss: 0.00123 | Correct: 64/64\n",
      "Time taken: 680.045 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ensemble.fit(\n",
    "    train_loader,\n",
    "    epochs=epochs,                # number of training epochs\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d9d2f2f-1e5a-430d-9fd8-cc302d8819ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the ensemble\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m acc \u001b[38;5;241m=\u001b[39m ensemble\u001b[38;5;241m.\u001b[39mevaluate(test_loader)  \u001b[38;5;66;03m# testing accuracy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torchensemble\\voting.py:305\u001b[0m, in \u001b[0;36mVotingClassifier.evaluate\u001b[1;34m(self, test_loader, return_loss)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;129m@torchensemble_model_doc\u001b[39m(item\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier_evaluate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_loader, return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(test_loader, return_loss)\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torchensemble\\_base.py:284\u001b[0m, in \u001b[0;36mBaseClassifier.evaluate\u001b[1;34m(self, test_loader, return_loss)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, elem \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[0;32m    282\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m split_data_target(elem, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 284\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m    286\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    288\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torchensemble\\voting.py:122\u001b[0m, in \u001b[0;36mVotingClassifier.forward\u001b[1;34m(self, *x)\u001b[0m\n\u001b[0;32m    116\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    117\u001b[0m     F\u001b[38;5;241m.\u001b[39msoftmax(op\u001b[38;5;241m.\u001b[39munsqueeze_tensor(estimator(\u001b[38;5;241m*\u001b[39mx)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[0;32m    119\u001b[0m ]\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoting_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 122\u001b[0m     proba \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39maverage(outputs)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     proba \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mmajority_vote(outputs)\n",
      "File \u001b[1;32m~\\Desktop\\Semester6\\Deep-Learning-Research-project\\env\\Lib\\site-packages\\torchensemble\\utils\\operator.py:22\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(outputs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage\u001b[39m(outputs):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the average over a list of tensors with the same size.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(outputs) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Evaluate the ensemble\n",
    "acc = ensemble.evaluate(test_loader)  # testing accuracy\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
