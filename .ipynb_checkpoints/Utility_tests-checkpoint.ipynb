{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2788198e-b6c7-4316-b659-842136c530ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "# Extras\n",
    "from matplotlib.patches import FancyArrowPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ef21aa-dc9b-4b07-9735-c7f9a9dba30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Use the first CUDA device\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32acfd-9e26-4d39-924e-484f750d02d0",
   "metadata": {},
   "source": [
    "# Data Generating Process, NOT USED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2338c0db-ebc5-4c1c-a580-649df20e2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "tensor([[[0.6130, 0.0101, 0.3984, 0.0403, 0.1563],\n",
      "         [0.4825, 0.7362, 0.4060, 0.5189, 0.2867],\n",
      "         [0.2416, 0.9228, 0.8299, 0.0342, 0.3879],\n",
      "         [0.0824, 0.7742, 0.2792, 0.5138, 0.2068]],\n",
      "\n",
      "        [[0.9877, 0.1289, 0.5621, 0.5221, 0.7445],\n",
      "         [0.5955, 0.9647, 0.8979, 0.7730, 0.6681],\n",
      "         [0.5462, 0.5071, 0.5882, 0.2555, 0.8963],\n",
      "         [0.1866, 0.7235, 0.1781, 0.9880, 0.6289]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n",
      "Batch 2:\n",
      "tensor([[[0.8259, 0.2128, 0.7292, 0.3209, 0.7550],\n",
      "         [0.2132, 0.9278, 0.1442, 0.3472, 0.9975],\n",
      "         [0.5531, 0.2491, 0.9302, 0.2792, 0.8427],\n",
      "         [0.5213, 0.1384, 0.6954, 0.5918, 0.8705]],\n",
      "\n",
      "        [[0.7296, 0.1194, 0.0511, 0.2384, 0.2015],\n",
      "         [0.6364, 0.7799, 0.5795, 0.5838, 0.0749],\n",
      "         [0.9054, 0.2269, 0.5483, 0.3980, 0.4130],\n",
      "         [0.7970, 0.0470, 0.9436, 0.1394, 0.6422]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n",
      "Batch 3:\n",
      "tensor([[[0.3264, 0.9967, 0.0706, 0.2893, 0.5037],\n",
      "         [0.1414, 0.5756, 0.8315, 0.4211, 0.1196],\n",
      "         [0.2742, 0.5458, 0.9813, 0.1816, 0.5561],\n",
      "         [0.2942, 0.3809, 0.4090, 0.5625, 0.8201]],\n",
      "\n",
      "        [[0.3148, 0.3071, 0.6121, 0.2254, 0.1048],\n",
      "         [0.2307, 0.2066, 0.0509, 0.2135, 0.7284],\n",
      "         [0.9660, 0.1951, 0.4424, 0.0735, 0.4682],\n",
      "         [0.9109, 0.2555, 0.7145, 0.9248, 0.6548]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_vector_batches(count: int, dim: int, n: int, device: torch.device, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Generates count batches of a tensor with shape (2, n, dim) on the specified device.\n",
    "    \n",
    "    Parameters:\n",
    "    count (int): Number of batches to generate.\n",
    "    dim (int): Dimension of each vector.\n",
    "    n (int): Number of vectors in each batch.\n",
    "    device (torch.device): The device on which to create the tensors.\n",
    "    seed (int, optional): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    List[torch.Tensor]: List of batches, each batch is a tensor with shape (2, n, dim).\n",
    "    \"\"\"\n",
    "\n",
    "    if(seed != None):\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    for _ in range(count):\n",
    "        # Generate the first list of n vectors in R^d on the specified device\n",
    "        vectors1 = torch.rand((n, dim), device=device)\n",
    "        \n",
    "        # Generate the second list of n vectors in R^d on the specified device\n",
    "        vectors2 = torch.rand((n, dim), device=device)\n",
    "        \n",
    "        # Stack the two lists along a new dimension to create a tensor of shape (2, n, dim)\n",
    "        batch = torch.stack((vectors1, vectors2), dim=0)\n",
    "        \n",
    "        # Add the tensor to the batches list\n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "\n",
    "# Generate batches with a specific seed\n",
    "seed = 42\n",
    "batches = generate_vector_batches(count = 3, dim = 5, n = 4, device = device, seed = seed)\n",
    "for i, batch in enumerate(batches):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(batch)\n",
    "    print(batch.shape)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9da0b-30a6-444a-a7a2-3eff15978271",
   "metadata": {},
   "source": [
    "* Duplicate batches checker (Needed because of seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9cab13-9fa9-43a9-99ad-e58da2db49e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_for_duplicates(batches):\n",
    "    \"\"\"\n",
    "    Checks for duplicate batches in the list.\n",
    "    \n",
    "    Parameters:\n",
    "    batches (List[torch.Tensor]): List of batches to check for duplicates.\n",
    "    \n",
    "    Returns:\n",
    "    List[int]: List of indices of duplicate batches.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        # Convert the tensor to a hashable type (e.g., a tuple)\n",
    "        batch_tuple = tuple(batch.cpu().numpy().ravel())\n",
    "        if batch_tuple in seen:\n",
    "            duplicates.append(i)\n",
    "        else:\n",
    "            seen.add(batch_tuple)\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "seed = 42\n",
    "check_for_duplicates(generate_vector_batches(count = 50000, dim = 2, n = 4, device = device, seed = seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5859169-8dc6-4596-ab1c-7d8bd41643c7",
   "metadata": {},
   "source": [
    "* No Duplicate generation detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa0ef4-41b9-4586-9e79-48f37bd18675",
   "metadata": {},
   "source": [
    "# Projection functions and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41d6b3-4339-4022-ad93-3d83cc9ea6b8",
   "metadata": {},
   "source": [
    "* ## Primal Projection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9931b0e-01f3-4289-a88c-0f4109b0093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "tensor([[ 1,  3,  5],\n",
      "        [ 4,  3,  2],\n",
      "        [ 5,  3, 35]])\n",
      "\n",
      "Projected matrix:\n",
      "tensor([[0.2426, 0.4393, 0.3180],\n",
      "        [0.4063, 0.3984, 0.1953],\n",
      "        [0.3511, 0.1622, 0.4867]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def softmax(matrix,axis = None,T=1):\n",
    "    exp_matrix = torch.exp(matrix/T - torch.max(matrix, dim=axis, keepdim=True).values)\n",
    "    softmax_matrix = exp_matrix / torch.sum(exp_matrix, dim=axis, keepdim=True)\n",
    "    return softmax_matrix\n",
    "\n",
    "\n",
    "def normalize_axis(to_normalize, axis=None):\n",
    "    axis_sum = torch.sum(to_normalize, dim=axis, keepdim=True)\n",
    "    \n",
    "    # Normalize only if the sum is greater than 1\n",
    "    if torch.any(axis_sum > 1):\n",
    "        return to_normalize / axis_sum\n",
    "    return to_normalize\n",
    "\n",
    "\n",
    "def normalize_matrix_axis(matrix,axis = None):\n",
    "    slices = [normalize_axis(to_normalize=matrix.select(axis, i), axis=None) for i in range(matrix.size(axis))]\n",
    "    return torch.stack(slices, dim=axis)\n",
    "\n",
    "\n",
    "def get_axis_sums(matrix,axis = None):\n",
    "    return torch.sum(matrix, dim=axis)\n",
    "\n",
    "\n",
    "def primal_projection(matrix,axis = None):\n",
    "    ret_mat = normalize_matrix_axis(matrix = softmax(matrix = matrix,axis = axis),axis = axis).clone()\n",
    "    r = get_axis_sums(ret_mat,axis = 1)\n",
    "    c = get_axis_sums(ret_mat,axis = 0)\n",
    "    r_tild = torch.ones(len(ret_mat), device=matrix.device) - r\n",
    "    c_tild = torch.ones(len(ret_mat[0]), device=matrix.device) - c\n",
    "    ret_mat = ret_mat + (1 / torch.sum(c_tild)) * torch.outer(r_tild, c_tild)\n",
    "    return ret_mat\n",
    "\n",
    "# Example usage\n",
    "matrix = torch.tensor(data = [[1,3,5],[4,3,2],[5,3,35]])  # Example random matrix\n",
    "projected_matrix = primal_projection(matrix = matrix, axis = 0) # Row example\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "print(\"\\nProjected matrix:\")\n",
    "print(projected_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f806fd-e283-43a2-9cb0-7d5a7099d7cd",
   "metadata": {},
   "source": [
    "* ## Dual Projection Methods\n",
    "* - Non symmetric version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6316ea64-6102-4927-b396-8ba0eeade861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_ret: tensor([3., 5., 7.])\n",
      "g_ret: tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "def dual_projection(f, g, cost):\n",
    "    \"\"\"\n",
    "    Perform dual projection to obtain f_ret and g_ret.\n",
    "    \n",
    "    Parameters:\n",
    "    f (torch.Tensor): A 1D tensor with the f values.\n",
    "    g (torch.Tensor): A 1D tensor with the g values.\n",
    "    cost (torch.Tensor): A 2D tensor with cost values where cost[i][k] is the cost value.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: f_ret tensor.\n",
    "    torch.Tensor: g_ret tensor.\n",
    "    \"\"\"\n",
    "    # Ensure input tensors are on the same device\n",
    "    device = f.device\n",
    "    \n",
    "    # Compute g_ret (which is simply g)\n",
    "    g_ret = g.clone()\n",
    "    \n",
    "    # Compute the cost matrix subtraction\n",
    "    cost_minus_g = cost - g.unsqueeze(1)  # Broadcasting to subtract g from each column in cost\n",
    "    \n",
    "    # Compute min_over_k(cost[i][k] - g[k])\n",
    "    min_cost_minus_g = torch.min(cost_minus_g, dim=1).values\n",
    "    \n",
    "    # Compute f_ret\n",
    "    f_ret = torch.minimum(f, min_cost_minus_g)\n",
    "    \n",
    "    return f_ret, g_ret\n",
    "\n",
    "# Example usage\n",
    "f = torch.tensor([10.0, 20.0, 30.0], dtype=torch.float32)\n",
    "g = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "cost = torch.tensor([[4.0, 5.0, 6.0],\n",
    "                     [7.0, 8.0, 9.0],\n",
    "                     [10.0, 11.0, 12.0]], dtype=torch.float32)\n",
    "\n",
    "f_ret, g_ret = dual_projection(f, g, cost)\n",
    "print(\"f_ret:\", f_ret)\n",
    "print(\"g_ret:\", g_ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7b87b-6ab8-4060-a7ea-ace8b8c4d7fe",
   "metadata": {},
   "source": [
    "# Actual Data Generating Process With Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae33649-ca59-4afe-9ddc-750931689fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "tensor([[[ 2.2592, -9.7988, -2.0317, -9.1938, -6.8747],\n",
      "         [-0.3505,  4.7249, -1.8804,  0.3780, -4.2658],\n",
      "         [-5.1684,  8.4568,  6.5983, -9.3162, -2.2421],\n",
      "         [-8.3522,  5.4836, -4.4156,  0.2767, -5.8636]],\n",
      "\n",
      "        [[-9.8527,  7.5006,  9.3644, -7.5977, -6.0559],\n",
      "         [-0.9949, -8.9777, -0.2158,  8.6620, -2.3851],\n",
      "         [-8.9830, -4.6750,  2.9892, -4.6503, -6.5341],\n",
      "         [ 7.1294, -4.4773, -5.8144, -8.5785,  5.4023]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n",
      "Batch 2:\n",
      "tensor([[[ 2.1998, -3.1125,  9.7038, -0.6165,  8.4068],\n",
      "         [ 4.2547, -7.8968,  6.2767, -7.8162,  2.1988],\n",
      "         [-6.4988,  6.8130,  7.4095,  4.5738, -4.9187],\n",
      "         [-6.6656,  4.5402, -2.0261, -4.6084,  6.0591]],\n",
      "\n",
      "        [[-1.7817, -7.0508, -9.5433, -5.3377, -5.4033],\n",
      "         [-2.0324, -0.3833, -3.7556,  2.4402, -2.8781],\n",
      "         [ 3.0226,  8.6095, -4.6067, -9.0785,  9.8505],\n",
      "         [-4.4235,  0.2146,  1.8899,  2.3293, -3.4174]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n",
      "Batch 3:\n",
      "tensor([[[ 6.0725, -0.0777,  3.9564,  2.8926,  8.1169],\n",
      "         [-3.4172, -6.2949, -2.9048,  4.9945, -4.3444],\n",
      "         [ 5.7777,  8.3255, -4.8862,  3.9013,  0.1671],\n",
      "         [-9.5192,  1.1880,  6.3938, -9.1586,  0.4286]],\n",
      "\n",
      "        [[-6.2192,  8.9874, -4.8087, -3.5166, -4.2002],\n",
      "         [-4.6894,  1.9291,  8.0802,  7.1088,  8.7035],\n",
      "         [-5.9561, -9.2308,  5.1179,  6.4561,  9.5359],\n",
      "         [ 7.6193,  2.1117, -0.6861,  6.3743,  1.6452]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_vector_batches(count: int, dim: int, n: int, device: torch.device, coord_max: float, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Generates a single tensor with shape (count, 2, n, dim) on the specified device,\n",
    "    with each coordinate having values in the range [-coord_max, coord_max].\n",
    "    \n",
    "    Parameters:\n",
    "    count (int): Number of batches to generate.\n",
    "    dim (int): Dimension of each vector.\n",
    "    n (int): Number of vectors in each batch.\n",
    "    device (torch.device): The device on which to create the tensors.\n",
    "    coord_max (float): The maximum absolute value for each coordinate.\n",
    "    seed (int, optional): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: A tensor with shape (count, 2, n, dim), with values in the range [-coord_max, coord_max].\n",
    "    \"\"\"\n",
    "    \n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate random values in the range [-coord_max, coord_max]\n",
    "    batches = (2 * torch.rand((count, 2, n, dim), device=device) - 1) * coord_max\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "coord_max = 10.0\n",
    "batches = generate_vector_batches(count=3, dim=5, n=4, device=device, coord_max=coord_max, seed=42)\n",
    "\n",
    "for i in range(batches.size(0)):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(batches[i])\n",
    "    print(batches[i].shape)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01349202-cc24-4e9b-ad84-920410c81ca1",
   "metadata": {},
   "source": [
    "# Wasserstein Distance verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf363c-d5c8-4002-92c7-41aa605f0104",
   "metadata": {},
   "source": [
    "* Reset Device to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26be8c0-722b-43ea-b0d7-aa6a8db33e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Use the first CUDA device\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1dfb1cf-3fa8-436c-b9c7-f3c3fecea9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def wass_permutations(batch, device):\n",
    "    \"\"\"\n",
    "    Given two lists of d-dimensional vectors, each of size n, computes the Wasserstein p=2 distance between them.\n",
    "    \n",
    "    This distance is found by checking every possible permutation of vector couplings and selecting the one \n",
    "    that minimizes the sum of squared distances between corresponding vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch: tensor of shape (2, n, d), where batch[0] and batch[1] are the two sets of n vectors to be compared.\n",
    "    - device: 'cpu' or 'cuda' for GPU computation.\n",
    "    \n",
    "    Returns:\n",
    "    - Wasserstein distance (p=2) between the two sets of vectors.\n",
    "    \"\"\"\n",
    "    # Move batch to specified device (CPU or GPU)\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    vectors1 = batch[0]\n",
    "    vectors2 = batch[1]\n",
    "    n = vectors1.shape[0]\n",
    "\n",
    "    # Generate all permutations of size n\n",
    "    all_permutations = list(itertools.permutations(range(n)))\n",
    "\n",
    "    min_squared_distance = float('inf')\n",
    "\n",
    "    # Loop over all permutations\n",
    "    for perm in all_permutations:\n",
    "        # Create an index tensor for the current permutation\n",
    "        perm_tensor = torch.tensor(perm, device=device)\n",
    "\n",
    "        # Compute the squared distance for this permutation\n",
    "        squared_distance_sum = torch.sum((vectors1 - vectors2[perm_tensor]) ** 2)\n",
    "\n",
    "        # Update minimum distance\n",
    "        if squared_distance_sum < min_squared_distance:\n",
    "            min_squared_distance = squared_distance_sum.item()  # Convert to Python float for comparison\n",
    "\n",
    "    # Return the Wasserstein distance (sqrt of minimum squared distance)\n",
    "    return torch.sqrt(torch.tensor(min_squared_distance, device=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13feac-2d79-4cad-8f03-81d7f0b82d47",
   "metadata": {},
   "source": [
    "## Sanity check \n",
    "* Should get sqrt(29) = 5.3852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50ad5b6-be32-4d32-8390-a6bbef0b5270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: tensor(5.3852, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vectors1 = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],  # e1\n",
    "    [0.0, 1.0, 0.0],  # e2\n",
    "    [0.0, 0.0, 1.0]   # e3\n",
    "])\n",
    "\n",
    "# Second set: {e1, 2e2, 3e3}\n",
    "vectors2 = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],  # e1\n",
    "    [0.0, 6.0, 0.0],  # 6e2\n",
    "    [0.0, 0.0, 3.0]   # 3e3\n",
    "])\n",
    "\n",
    "batch = torch.stack([vectors1, vectors2])\n",
    "\n",
    "# Calculate the Wasserstein distance\n",
    "distance = wass_permutations(batch,device = device)\n",
    "print(\"Wasserstein distance:\", distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29f75db0-013e-4e0f-856b-cf10f580bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def wass_hungarian(batch):\n",
    "    \"\"\"\n",
    "    Given two sets of d-dimensional vectors, each of size n, computes the Wasserstein p=2 distance between them.\n",
    "    \n",
    "    This distance is found by using the Hungarian algorithm to find the optimal matching \n",
    "    that minimizes the sum of squared distances between corresponding vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch: tensor of shape (2, n, d), where batch[0] and batch[1] are the two sets of n vectors to be compared.\n",
    "    \n",
    "    Returns:\n",
    "    - Wasserstein distance (p=2) between the two sets of vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the two sets of vectors\n",
    "    vectors1 = batch[0]  # Shape: (n, d)\n",
    "    vectors2 = batch[1]  # Shape: (n, d)\n",
    "\n",
    "    # Compute the squared Euclidean distance matrix using broadcasting\n",
    "    distance_matrix = torch.sum((vectors1[:, None, :] - vectors2[None, :, :]) ** 2, dim=-1)\n",
    "\n",
    "    # Use the Hungarian algorithm to find the optimal assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix.cpu().numpy())\n",
    "\n",
    "    # Calculate the total minimum distance\n",
    "    min_squared_distance = distance_matrix[row_ind, col_ind].sum()\n",
    "\n",
    "    # Return the Wasserstein distance (p=2)\n",
    "    return torch.sqrt(min_squared_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effd098-ab50-442e-bbe5-e7feb69846da",
   "metadata": {},
   "source": [
    "## Timing each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd341c9-05ef-40f0-ad42-74297867c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "seed = 16\n",
    "count = 1000\n",
    "dim = 10\n",
    "n = 4\n",
    "coordinate_max = 5\n",
    "batches = generate_vector_batches(count=count,\n",
    "                                            dim=dim,\n",
    "                                            n=n, \n",
    "                                            device=device,\n",
    "                                            seed=seed,\n",
    "                                            coord_max=coordinate_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0048626-6f84-4afe-8bdf-19145f2cad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (permutations): 3.340263843536377\n"
     ]
    }
   ],
   "source": [
    "# O(n!*n)\n",
    "# Time the permutations implementation\n",
    "start_time = time.time()\n",
    "for i in range(batches.shape[0]):\n",
    "    wass_permutations(batches[i],device=device)\n",
    "\n",
    "print(\"Time taken (permutations):\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1ec95d-33ca-4bff-afa1-c71c2ecb64c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (Hungarian): 0.24085545539855957\n"
     ]
    }
   ],
   "source": [
    "# O(n^3)\n",
    "# Time the Hungarian implementation\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(batches.shape[0]):\n",
    "    wass_hungarian(batches[i])\n",
    "\n",
    "print(\"Time taken (Hungarian):\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ddc5f-184d-4df7-99ec-cb6aaace0652",
   "metadata": {},
   "source": [
    "# Primal Dual solution bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d22e706-0afb-4952-9578-7df73c786585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(vecs1, vecs2):\n",
    "    \"\"\"\n",
    "    Computes the pairwise distance matrix between two sets of vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - vecs1: Tensor of shape (n, d), where n is the number of vectors and d is the dimension of each vector.\n",
    "    - vecs2: Tensor of shape (n, d), where m is the number of vectors and d is the dimension of each vector.\n",
    "\n",
    "    Returns:\n",
    "    - distance_matrix: Tensor of shape (n, n), where each element [i, j] represents the Euclidean distance between vecs1[i] and vecs2[j].\n",
    "    \"\"\"\n",
    "    # Compute the pairwise distance matrix using torch.cdist (Euclidean distance by default)\n",
    "    distance_matrix = torch.cdist(vecs1, vecs2, p=2)\n",
    "    \n",
    "    return distance_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbcde3-e033-48c1-941a-a06a164e90cb",
   "metadata": {},
   "source": [
    "* Generating toy batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84996ebf-118e-4cf6-ad50-1b7bb57be79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: tensor(5.3852, device='cuda:0')\n",
      "Distances matrix: \n",
      "tensor([[0.0000, 6.0828, 3.1623],\n",
      "        [1.4142, 5.0000, 3.1623],\n",
      "        [1.4142, 6.0828, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "vectors1 = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],  # e1\n",
    "    [0.0, 1.0, 0.0],  # e2\n",
    "    [0.0, 0.0, 1.0]   # e3\n",
    "])\n",
    "\n",
    "# Second set: {e1, 2e2, 3e3}\n",
    "vectors2 = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],  # e1\n",
    "    [0.0, 6.0, 0.0],  # 6e2\n",
    "    [0.0, 0.0, 3.0]   # 3e3\n",
    "])\n",
    "\n",
    "batch = torch.stack([vectors1, vectors2])\n",
    "\n",
    "# Calculate the Wasserstein distance\n",
    "distance = wass_permutations(batch,device=device)\n",
    "print(\"Wasserstein distance:\", distance)\n",
    "distances_mat = compute_distance_matrix(vecs1=vectors1,vecs2 = vectors2)\n",
    "print(\"Distances matrix: \")\n",
    "print(distances_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8953febc-1bdc-4eb7-9797-7e8ac960e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def primal_sol(out_put_mat,distances_mat):\n",
    "    # Flatten the matrices\n",
    "    bi_stoch_mat = primal_projection(out_put_mat,axis = 0)\n",
    "    B = bi_stoch_mat.view(-1)\n",
    "    D = (distances_mat**2).view(-1)\n",
    "    \n",
    "    # Compute the dot product of the flattened matrices\n",
    "    dot_product = torch.dot(B, D)\n",
    "    return torch.sqrt(dot_product)\n",
    "\n",
    "\n",
    "def dual_sol(f,g,distances_mat):\n",
    "    projected_f,projected_g = dual_projection(f=f, g=g , cost =distances_mat)\n",
    "    return torch.sum(projected_f + projected_g) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "793992e2-fc0d-4b1d-a807-18088ef1de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "out_matrix = torch.rand(3, 3)\n",
    "f = torch.rand(3)\n",
    "g = torch.rand(3)\n",
    "sol_prim = primal_sol(out_put_mat=out_matrix,distances_mat=distances_mat)\n",
    "sol_dual = dual_sol(f=f,g=g,distances_mat=distances_mat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e47319f9-4e73-4c4c-a5f3-5bd2458a92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual solution 2.748887062072754\n",
      "actual value 5.385164737701416\n",
      "Primal solution 6.526750564575195\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dual solution {sol_dual}\")\n",
    "print(f\"actual value {distance}\")\n",
    "print(f\"Primal solution {sol_prim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b267b9-9536-4dca-a68e-342c31b28c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_check(distances_mat,actual_distance,device,n=1000):\n",
    "    prim_sols = -torch.ones(n)\n",
    "    dual_sols = -torch.ones(n)\n",
    "    for i in range(n):\n",
    "        out_matrix = torch.rand(3, 3)\n",
    "        f = torch.rand(3)\n",
    "        g = torch.rand(3)\n",
    "        sol_prim = primal_sol(out_put_mat=out_matrix,distances_mat=distances_mat)\n",
    "        sol_dual = dual_sol(f=f,g=g,distances_mat=distances_mat) \n",
    "        prim_sols[i] = sol_prim\n",
    "        dual_sols[i] = sol_dual \n",
    "    print(f\"Max Dual solution {dual_sols.max()}\")\n",
    "    print(f\"actual value {distance}\")\n",
    "    print(f\"Min Primal solution {prim_sols.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1bde96-2ed7-4088-bb37-0622717985d0",
   "metadata": {},
   "source": [
    "### Fair Enough we think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a81b0874-21f1-40d4-bdc2-a06e89f83b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Dual solution 2.8284270763397217\n",
      "actual value 5.385164737701416\n",
      "Min Primal solution 6.230830192565918\n",
      "Time taken: 8.164591550827026\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "random_check(distances_mat = distances_mat,actual_distance=distance,device=device, n = 30000)\n",
    "print(\"Time taken:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981def3-9e1f-47cf-a259-bdf9fd254e70",
   "metadata": {},
   "source": [
    "### Checking if both methods are accurate to eachother up to numeric tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f89a49d-3aa6-49d6-9529-1ef5cd6b37db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1e-05 tolerance:\n",
      "All batches produce equivalent results.\n"
     ]
    }
   ],
   "source": [
    "def check_wasserstein_equivalence(batches, device, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Checks if the outputs of wass_permutations and wass_hungarian functions are the same \n",
    "    for a list of batches, up to a numeric tolerance of epsilon.\n",
    "\n",
    "    Parameters:\n",
    "    - batches: A list of input tensors, each of shape (2, n, d), for both functions.\n",
    "    - device: The device on which to run the computation ('cpu' or 'cuda').\n",
    "    - epsilon: Numeric tolerance for comparing the results.\n",
    "\n",
    "    Returns:\n",
    "    - True if all outputs are within the tolerance epsilon, False otherwise.\n",
    "    \"\"\"\n",
    "    all_equivalent = True\n",
    "    \n",
    "    for idx, batch in enumerate(batches):\n",
    "        # Move the batch to the specified device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Calculate wasserstein distance using both methods\n",
    "        wass_perm_result = wass_permutations(batch, device)\n",
    "        wass_hung_result = wass_hungarian(batch)\n",
    "        \n",
    "        # Compare the results up to the specified tolerance\n",
    "        difference = torch.abs(wass_perm_result - wass_hung_result)\n",
    "        \n",
    "        if difference >= epsilon:\n",
    "            print(f\"Batch {idx}: Results differ by {difference.item()}, which is larger than epsilon = {epsilon}.\")\n",
    "            all_equivalent = False\n",
    "    \n",
    "    return all_equivalent\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 16\n",
    "count = 2000\n",
    "dim = 6\n",
    "n = 4\n",
    "coordinate_max = 5\n",
    "batches= generate_vector_batches(count=count,\n",
    "                                            dim=dim,\n",
    "                                            n=n, \n",
    "                                            device=device,\n",
    "                                            seed=seed,\n",
    "                                            coord_max=coordinate_max)\n",
    "\n",
    "# Check if the two functions give equivalent results for all batches\n",
    "epsilon = 1e-5\n",
    "all_batches_equivalent = check_wasserstein_equivalence(batches, device,epsilon=epsilon)\n",
    "print(f\"For {epsilon} tolerance:\")\n",
    "if all_batches_equivalent:\n",
    "    print(\"All batches produce equivalent results.\")\n",
    "else:\n",
    "    print(\"Some batches produced differing results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196cd84-f508-4001-9f39-bd0a24a9561a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
