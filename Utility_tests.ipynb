{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2788198e-b6c7-4316-b659-842136c530ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ef21aa-dc9b-4b07-9735-c7f9a9dba30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Use the first CUDA device\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32acfd-9e26-4d39-924e-484f750d02d0",
   "metadata": {},
   "source": [
    "# Data Generating Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2338c0db-ebc5-4c1c-a580-649df20e2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "tensor([[[0.6130, 0.0101, 0.3984, 0.0403, 0.1563],\n",
      "         [0.4825, 0.7362, 0.4060, 0.5189, 0.2867],\n",
      "         [0.2416, 0.9228, 0.8299, 0.0342, 0.3879],\n",
      "         [0.0824, 0.7742, 0.2792, 0.5138, 0.2068]],\n",
      "\n",
      "        [[0.9877, 0.1289, 0.5621, 0.5221, 0.7445],\n",
      "         [0.5955, 0.9647, 0.8979, 0.7730, 0.6681],\n",
      "         [0.5462, 0.5071, 0.5882, 0.2555, 0.8963],\n",
      "         [0.1866, 0.7235, 0.1781, 0.9880, 0.6289]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n",
      "Batch 2:\n",
      "tensor([[[0.8259, 0.2128, 0.7292, 0.3209, 0.7550],\n",
      "         [0.2132, 0.9278, 0.1442, 0.3472, 0.9975],\n",
      "         [0.5531, 0.2491, 0.9302, 0.2792, 0.8427],\n",
      "         [0.5213, 0.1384, 0.6954, 0.5918, 0.8705]],\n",
      "\n",
      "        [[0.7296, 0.1194, 0.0511, 0.2384, 0.2015],\n",
      "         [0.6364, 0.7799, 0.5795, 0.5838, 0.0749],\n",
      "         [0.9054, 0.2269, 0.5483, 0.3980, 0.4130],\n",
      "         [0.7970, 0.0470, 0.9436, 0.1394, 0.6422]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n",
      "Batch 3:\n",
      "tensor([[[0.3264, 0.9967, 0.0706, 0.2893, 0.5037],\n",
      "         [0.1414, 0.5756, 0.8315, 0.4211, 0.1196],\n",
      "         [0.2742, 0.5458, 0.9813, 0.1816, 0.5561],\n",
      "         [0.2942, 0.3809, 0.4090, 0.5625, 0.8201]],\n",
      "\n",
      "        [[0.3148, 0.3071, 0.6121, 0.2254, 0.1048],\n",
      "         [0.2307, 0.2066, 0.0509, 0.2135, 0.7284],\n",
      "         [0.9660, 0.1951, 0.4424, 0.0735, 0.4682],\n",
      "         [0.9109, 0.2555, 0.7145, 0.9248, 0.6548]]], device='cuda:0')\n",
      "torch.Size([2, 4, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_vector_batches(count: int, dim: int, n: int, device: torch.device, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Generates count batches of a tensor with shape (2, n, dim) on the specified device.\n",
    "    \n",
    "    Parameters:\n",
    "    count (int): Number of batches to generate.\n",
    "    dim (int): Dimension of each vector.\n",
    "    n (int): Number of vectors in each batch.\n",
    "    device (torch.device): The device on which to create the tensors.\n",
    "    seed (int, optional): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    List[torch.Tensor]: List of batches, each batch is a tensor with shape (2, n, dim).\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    for _ in range(count):\n",
    "        # Generate the first list of n vectors in R^d on the specified device\n",
    "        vectors1 = torch.rand((n, dim), device=device)\n",
    "        \n",
    "        # Generate the second list of n vectors in R^d on the specified device\n",
    "        vectors2 = torch.rand((n, dim), device=device)\n",
    "        \n",
    "        # Stack the two lists along a new dimension to create a tensor of shape (2, n, dim)\n",
    "        batch = torch.stack((vectors1, vectors2), dim=0)\n",
    "        \n",
    "        # Add the tensor to the batches list\n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "\n",
    "# Generate batches with a specific seed\n",
    "seed = 42\n",
    "batches = generate_vector_batches(count = 3, dim = 5, n = 4, device = device, seed = seed)\n",
    "for i, batch in enumerate(batches):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(batch)\n",
    "    print(batch.shape)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9da0b-30a6-444a-a7a2-3eff15978271",
   "metadata": {},
   "source": [
    "* Duplicate batches checker (Needed because of seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9cab13-9fa9-43a9-99ad-e58da2db49e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_for_duplicates(batches):\n",
    "    \"\"\"\n",
    "    Checks for duplicate batches in the list.\n",
    "    \n",
    "    Parameters:\n",
    "    batches (List[torch.Tensor]): List of batches to check for duplicates.\n",
    "    \n",
    "    Returns:\n",
    "    List[int]: List of indices of duplicate batches.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        # Convert the tensor to a hashable type (e.g., a tuple)\n",
    "        batch_tuple = tuple(batch.cpu().numpy().ravel())\n",
    "        if batch_tuple in seen:\n",
    "            duplicates.append(i)\n",
    "        else:\n",
    "            seen.add(batch_tuple)\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "seed = 42\n",
    "check_for_duplicates(generate_vector_batches(count = 50000, dim = 2, n = 4, device = device, seed = seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5859169-8dc6-4596-ab1c-7d8bd41643c7",
   "metadata": {},
   "source": [
    "* No Duplicate generation detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa0ef4-41b9-4586-9e79-48f37bd18675",
   "metadata": {},
   "source": [
    "# Projection functions and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41d6b3-4339-4022-ad93-3d83cc9ea6b8",
   "metadata": {},
   "source": [
    "* ## Primal Projection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1fab4c-3e1c-4fbb-96f0-a0c16525a4dc",
   "metadata": {},
   "source": [
    "Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259d3dd5-7857-4ade-8494-5d8cc91589cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax(matrix):\n",
    "#     # Subtract the max for numerical stability\n",
    "#     exp_matrix = np.exp(matrix - np.max(matrix, axis=0, keepdims=True))\n",
    "#     # Apply softmax\n",
    "#     softmax_matrix = exp_matrix / np.sum(exp_matrix, axis=0, keepdims=True)\n",
    "#     return softmax_matrix\n",
    "# # --------------------------------------------\n",
    "# def normalize_row(row):\n",
    "#     row_sum = np.sum(row)\n",
    "#     if row_sum > 1:\n",
    "#         return row / row_sum\n",
    "#     return row\n",
    "# # --------------------------------------------\n",
    "# def normalize_matrix_rows(matrix):\n",
    "#     # Apply normalization to each row\n",
    "#     normalized_mat = np.array([normalize_row(row) for row in matrix])\n",
    "#     return normalized_mat\n",
    "# # --------------------------------------------\n",
    "# def get_row_sums(matrix):\n",
    "#     return np.sum(matrix, axis=1)\n",
    "# def get_col_sums(matrix):\n",
    "#     return np.sum(matrix, axis=0)\n",
    "# # --------------------------------------------\n",
    "# def primal_projection(matrix):\n",
    "#     ret_mat = normalize_matrix_rows(softmax(matrix)).copy()\n",
    "#     r = get_row_sums(ret_mat)\n",
    "#     c = get_col_sums(ret_mat)\n",
    "#     r_tild = np.ones(len(ret_mat)) - r\n",
    "#     c_tild = np.ones(len(ret_mat)) - c\n",
    "#     # Actual computation\n",
    "#     ret_mat = ret_mat + (1/np.sum(c_tild))*np.outer(r_tild,c_tild)\n",
    "#     return ret_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9931b0e-01f3-4289-a88c-0f4109b0093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "tensor([[1, 5],\n",
      "        [3, 2]])\n",
      "\n",
      "Projected matrix:\n",
      "tensor([[0.1112, 0.8888],\n",
      "        [0.8888, 0.1112]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def softmax(matrix,axis = None):\n",
    "    exp_matrix = torch.exp(matrix - torch.max(matrix, dim=axis, keepdim=True).values)\n",
    "    softmax_matrix = exp_matrix / torch.sum(exp_matrix, dim=axis, keepdim=True)\n",
    "    return softmax_matrix\n",
    "\n",
    "\n",
    "def normalize_axis(to_normalize, axis=None):\n",
    "    axis_sum = torch.sum(to_normalize, dim=axis, keepdim=True)\n",
    "    \n",
    "    # Normalize only if the sum is greater than 1\n",
    "    if torch.any(axis_sum > 1):\n",
    "        return to_normalize / axis_sum\n",
    "    return to_normalize\n",
    "\n",
    "\n",
    "def normalize_matrix_axis(matrix,axis = None):\n",
    "    slices = [normalize_axis(to_normalize=matrix.select(axis, i), axis=None) for i in range(matrix.size(axis))]\n",
    "    return torch.stack(slices, dim=axis)\n",
    "\n",
    "\n",
    "def get_axis_sums(matrix,axis = None):\n",
    "    return torch.sum(matrix, dim=axis)\n",
    "\n",
    "\n",
    "def primal_projection(matrix,axis = None):\n",
    "    ret_mat = normalize_matrix_axis(matrix = softmax(matrix = matrix,axis = axis),axis = axis).clone()\n",
    "    r = get_axis_sums(ret_mat,axis = 1)\n",
    "    c = get_axis_sums(ret_mat,axis = 0)\n",
    "    r_tild = torch.ones(len(ret_mat), device=matrix.device) - r\n",
    "    c_tild = torch.ones(len(ret_mat[0]), device=matrix.device) - c\n",
    "    ret_mat = ret_mat + (1 / torch.sum(c_tild)) * torch.outer(r_tild, c_tild)\n",
    "    return ret_mat\n",
    "\n",
    "# Example usage\n",
    "matrix = torch.tensor(data = [[1,5],[3,2]])  # Example random matrix\n",
    "projected_matrix = primal_projection(matrix = matrix, axis = 0) # Row example\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "print(\"\\nProjected matrix:\")\n",
    "print(projected_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f806fd-e283-43a2-9cb0-7d5a7099d7cd",
   "metadata": {},
   "source": [
    "* ## Dual Projection Methods\n",
    "* - Non symmetric version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6316ea64-6102-4927-b396-8ba0eeade861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_ret: tensor([3., 5., 7.])\n",
      "g_ret: tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "def dual_projection(f, g, cost):\n",
    "    \"\"\"\n",
    "    Perform dual projection to obtain f_ret and g_ret.\n",
    "    \n",
    "    Parameters:\n",
    "    f (torch.Tensor): A 1D tensor with the f values.\n",
    "    g (torch.Tensor): A 1D tensor with the g values.\n",
    "    cost (torch.Tensor): A 2D tensor with cost values where cost[i][k] is the cost value.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: f_ret tensor.\n",
    "    torch.Tensor: g_ret tensor.\n",
    "    \"\"\"\n",
    "    # Ensure input tensors are on the same device\n",
    "    device = f.device\n",
    "    \n",
    "    # Compute g_ret (which is simply g)\n",
    "    g_ret = g.clone()\n",
    "    \n",
    "    # Compute the cost matrix subtraction\n",
    "    cost_minus_g = cost - g.unsqueeze(1)  # Broadcasting to subtract g from each column in cost\n",
    "    \n",
    "    # Compute min_over_k(cost[i][k] - g[k])\n",
    "    min_cost_minus_g = torch.min(cost_minus_g, dim=1).values\n",
    "    \n",
    "    # Compute f_ret\n",
    "    f_ret = torch.minimum(f, min_cost_minus_g)\n",
    "    \n",
    "    return f_ret, g_ret\n",
    "\n",
    "# Example usage\n",
    "f = torch.tensor([10.0, 20.0, 30.0], dtype=torch.float32)\n",
    "g = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "cost = torch.tensor([[4.0, 5.0, 6.0],\n",
    "                     [7.0, 8.0, 9.0],\n",
    "                     [10.0, 11.0, 12.0]], dtype=torch.float32)\n",
    "\n",
    "f_ret, g_ret = dual_projection(f, g, cost)\n",
    "print(\"f_ret:\", f_ret)\n",
    "print(\"g_ret:\", g_ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fa5c8-df36-4871-9195-7abdf43de5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
